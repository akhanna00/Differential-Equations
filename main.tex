\documentclass{report}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{tcolorbox}
\usepackage{bm}
\tcbuselibrary{theorems}
\usepackage{physics}
\usepackage{graphicx}



% Things I use, but too lazy to find how again

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Make a hyperlink to another section:
% \hyperref[sec:hello]{Word of text} (set link to hello)

% \section{Hello World} 
% \label{sec:hello} (define a label for the section
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Make an unordered list:
% \begin{itemize}
%   \item 
%   \item
% \end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Make an ordered list:
% \begin{enumerate}
%   \item 
%   \item
% \end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Make a matrix
%\begin{bmatrix}
%        x_1y_1 & x_1y_2 & \dots  & x_1y_m \\
%        x_2y_1 & x_2y_2 & \dots  & x_2y_m \\
%        \vdots & \vdots & \ddots & \vdots \\
%        x_ny_1 & a_{m2} & \dots  & x_ny_m \\
%   \end{bmatrix}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Make an equation:
% \begin{equation}
% ...
% \end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Theorems:
%\begin{mytheo}{Title}{NameOfTheorem}
% ...
%\end{mytheo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% N'th order linear equation
% $$y^{(n)}(t) + ... + p_1(t)y'(t) + p_0(t)y(t) = 0$$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newtcbtheorem[number within=section]{mytheo}{Theorem}
{colback=orange!5,colframe=red!35!black,fonttitle=\bfseries}{th}



\hypersetup{
    pdftitle={Differential Equations},
    pdfpagemode=FullScreen,
    linkbordercolor = {red},
}




\title{Differential Equations}
\author{Arun Khanna}
\date{}

\begin{document}

\maketitle
\tableofcontents

\chapter{Types of Differential Equations}
Differential equations come in many types. This is a reference to look back into if you forget.
\section{Order of a differential equation}
The order of a differential equation is simply the highest derivative term in it, once it has been simplified as much as possible. For example:
$$y'' + 5y + 3 = 0$$
is a second order differential equation.

\section{}



\chapter{Existence of Solutions in Linear Differential Equations}
\section{Introduction}
Linear differential equations have interesting properties that allow us to solve them in convenient ways.

What does it mean to be linear anyway? For matrices, a linear transformation is simplify defined to be a transformation such that the even spacing of the grid-lines is preserved. However, generally, a function or transformation is defined to be linear if and only if it satisfies the following properties.

\begin{mytheo}{Linearity of a transformation}{lin}
    A transformation $T$ is defined to be linear if for inputs $\mathbf{v_1}$ and $\mathbf{v_2}$
    \begin{enumerate}
        \item $T(\mathbf{v_1}+\mathbf{v_2}) = T(\mathbf{v_1}) + T(\mathbf{v_2})$
        \item $T(c\mathbf{v_1}) = cT(\mathbf{v_1})$, where $c$ is a scalar constant
   \end{enumerate}
\end{mytheo}

A fascinating result of calculus is that derivatives are linear because they satisfy these two properties! Thus we can examine the linearity (or lack thereof) of differential equations, which are just equations connecting the various derivatives of the function together.

What does it mean to say that a differential equation is linear? It means that given two functions $y_1(t)$ and $y_2(t)$ satisfying the differential equation, that $y(t) = c_1y_1(t) + c_2y_2(t)$ should also satisfy the differential equation. It turns out that this condition is equivalent to having a differential equation in the form of:

\begin{mytheo}{Linearity of homogeneous differential equations}{lin_diff}
    A homogeneous differential equation is linear if and only if:
    $$y^{(n)}(t) + ... + p_1(t)y'(t) + p_0(t)y(t) = 0$$
\end{mytheo}


where all the coefficients of the derivatives of y are only functions of t and not y. For example, this would NOT be a linear differential equation:

$$y'' + 2yy' = 0$$
because of the $2y$ coefficient on $y'$.

What does having no coefficients dependent on y have anything to do with linearity? We know that all the derivative terms ($y^(n), ..., y'', y', y)$ must satisfy the definition of linearity because the derivative is a linear operator. However, if the coefficient is dependent on y in any way, adding two different potential solutions $y_1, y_2$ will NOT give us the same thing as adding up the individual solutions. We can see that in the example above:
$$(y_1+y_2)''+2(y_1+y_2)(y_1+y_2)'+3 = y_1'' + y_2'' + 2y_1y_1'+2y_1y_2' +2y_2y_1'+2y_2y_2'$$
$$=(y_1''+2y_1y_1')+ (y_2''+2y_2y_2')+2y_2y_1+2y_1y_2$$

Notice how taking $y = y_1+y_2$ and plugging it into the equation gives us the same thing as summing the individual results of plugging in $y_1$ and $y_2$, but with the additional garbage of $2y_2y_1+2y_1y_2$

\begin{mytheo}{Existence and Uniqueness Theorem}{Ex&Un}
    Given an n'th order linear differential equation
    $$y^{(n)}(t) + ... + p_1(t)y'(t) + p_0(t)y(t) = g(t)$$
    and n initial conditions at a time period $t_0$ inan interval $I$ where $p_{n-1}(t)$, $\dots$, $p_1(t)$, and $p_0(t)$ is continuous
    
    $y(t_0) = a_0$, $y'(t_0) = a_1$, \dots, $y^{(n)}(t_0) = a_n$
    
    there exists only one unique solution $y(t) = \phi(t)$ over the interval I.
    
\end{mytheo}


\section{Principle of Superposition}

\begin{mytheo}{Principle of Superposition}{super}
 If we have a homogeneous linear differential equation of the form:
 $$y^{(n)}(t) + ... + p_1(t)y'(t) + p_0(t)y(t) = 0$$
 and n solutions, $y_1(t)$, $y_2(t)$, $\dots$, $y_n(t)$, then any linear combination $c_1y_1(t) + c_2y_2(t) + \dots + c_ny_n(t)$ is also a solution.
\end{mytheo}


Why?
Well, a homogeneous equation of degree $m$ is in the form:
$$L[y] = y^{(n)}(t) + ... + p(t)y'(t) + q(t)y(t) = 0$$
We know that since this is linear, $L[y_i+y_j] = L[y_i] + L[y_j]$ and $L[cy_i] = cL[y_i]$ for a constant $c$.

If $y_1, ..., y_n$ are solutions, they must all satisfy $L[y_i] = 0$. Thus,
$$L[c_1y_1+c_2y_2+...+c_ny_n] = c_1L[y_1] + c_2L[y_2] + ... + c_nL[y_n] = 0 + 0 + ... + 0 = 0$$
Thus, the linear combination satisfies the condition to be a solution for this differential equation.



\section{Unique Homogeneous Solution and the Wronskian}
In this section, we will consider second order homogeneous equations for simplicity, however, the results apply in general.

By \hyperref[th:Ex&Un]{The Existence and Uniqueness Theorem}, we know that for an n'th order linear homogeneous differential equation with n initial conditions that there exists one unique solution. How do we go about finding this one unique solution?

Say that we've found two solutions $y_1(t)$ and $y_2(t)$ to the differential equation:

$$y'' + p_1(t)y' + p_0(t)y = 0$$

$$\text{with } y(t_0) = a_0 \text{ and } y'(t_0) = a_1$$

By \hyperref[th:super]{superposition}, we know that any linear combination of $y_1(t)$ and $y_2(t)$ is a solution.
$$y(t) = c_1y_1(t) + c_2y_2(t)$$
By our initial conditions:
$$y(t_0) = c_1y_1(t_0) + c_2y_2(t_0) = a_0$$
$$y'(t_0) = c_1y'_1(t_0) + c_2y'_2(t_0) = a_1$$

Since the \hyperref[th:Ex&Un]{The Existence and Uniqueness Theorem} tells us there is only one solution satisfying the initial constraints, we should be able to get unique values for the coefficients of the equation $c_1$ and $c_2$. Representing the series of equations in matrix form:
$$
\begin{bmatrix}
    y_1(t_0) & y_2(t_0) \\
    y_1'(t_0) & y_2'(t_0) \\
\end{bmatrix}
\begin{bmatrix}
    c_1 \\
    c_2 \\
\end{bmatrix}
=
\begin{bmatrix}
    a_0 \\
    a_1 \\
\end{bmatrix}
$$

The only way we are able to get values for $c_1$ and $c_2$ that satisfy this equation is if the matrix 
$\begin{bmatrix}
    y_1(t_0) & y_2(t_0) \\
    y_1'(t_0) & y_2'(t_0) \\
\end{bmatrix}$,
also known as the \textbf{Fundamental Matrix} is invertible. An equivalent condition to invertibilitity of a matrix is that its determinant is non-zero(columns are linearly independent). Thus provided that

$$\text{det}\left(\begin{bmatrix}
    y_1(t_0) & y_2(t_0) \\
    y_1'(t_0) & y_2'(t_0) \\
\end{bmatrix}\right)
\neq 0
$$
we can always find a solution for the coefficients $c_1$ and $c_2$
$$
\begin{bmatrix}
    c_1 \\
    c_2 \\
\end{bmatrix}
=
\begin{bmatrix}
    y_1(t_0) & y_2(t_0) \\
    y_1'(t_0) & y_2'(t_0) \\
\end{bmatrix}^{-1}
\begin{bmatrix}
    a_0 \\
    a_1 \\
\end{bmatrix}
$$

The determinant of the fundamental matrix is called the \textbf{Wronskian} and has a number of interesting properties. 

Thus, we can conclude that in general:

\begin{mytheo}{Unique Homogeneous Solution}{unHomSol}
 Given a homogeneous equation of the form:
  $$y^{(n)}(t) + ... + p_1(t)y'(t) + p_0(t)y(t) = 0$$
  and $n$ solutions $y_1(t)$, $y_2(t)$, $\dots$, $y_n(t)$ such that there exists a $t_0$ where the determinant of the fundamental matrix (the Wronskian) is non-zero:
  $$\text{det}\left(\begin{bmatrix}
    y_1(t_0) & y_2(t_0) & \dots  & y_n(t_0) \\
    y_1'(t_0) & y_2'(t_0) & \dots  & y_n'(t_0) \\
    \vdots & \vdots & \ddots & \vdots \\
    y_1^{(n)}(t_0) & y_2^{(n)}(t_0) & \dots  & y_n^{(n)}(t_0) \\
    \end{bmatrix}\right)
    \neq 0
    $$
 The general unique solution to the differential equation is in the form of:
 $$\phi(t) = c_1y_1(t) + c_2y_2(t) + \dots + c_ny_n(t)$$
 where $c_1, \dots, c_n$ can be determined given $n$ conditions of the form:
 $y(t_0) = a_0$, $y'(t_0) = a_1$, $\dots$, $y^{(n-1)}(t_0) = a_n$ by 
 $$
\begin{bmatrix}
    c_1 \\
    c_2 \\
    \vdots \\
    c_n
\end{bmatrix}
=
\begin{bmatrix}
    y_1(t_0) & y_2(t_0) & \dots  & y_n(t_0) \\
    y_1'(t_0) & y_2'(t_0) & \dots  & y_n'(t_0) \\
    \vdots & \vdots & \ddots & \vdots \\
    y_1^{(n)}(t_0) & y_2^{(n)}(t_0) & \dots  & y_n^{(n)}(t_0) \\
    \end{bmatrix}^{-1}
\begin{bmatrix}
    a_0 \\
    a_1 \\
    \vdots \\ 
    a_n
\end{bmatrix}
$$
\end{mytheo}

\section{Linear Independence and the Wronskian}
\section{Linear Dependence}
What does it mean for a function to be \textbf{linearly dependent} on another function? 

Recall what linear dependence means for vectors. When we have two vectors that are linearly dependant, we can represent one vector as a scaled version of the other:

[Insert Picture]

Algebraically, for two vectors $\bm{\vec{v_1}}$ and $\bm{\vec{v_2}}$ are \textbf{linearly independent} when:
$$\bm{\vec{v_1}} = c\bm{\vec{v_2}}$$
for some constant $c$

\textbf{Linear dependence} between two vectors occurs when this is not true, it is impossible to recreate one of the two linearly dependent vectors by just scaling the other one:

[Insert Picture]

Algebraically, for two vectors $\bm{\vec{v_1}}$ and $\bm{\vec{v_2}}$ are \textbf{linearly independent} when:
$$\bm{\vec{v_1}} \neq c\bm{\vec{v_2}}$$
for any constant $c$

When dealing with functions, the definitions of linearly dependence and independence are really similar:

\begin{mytheo}{Linear Dependence and Independence of functions}{lin_func}
    Two functions $f(t)$ and $g(t)$ are \textbf{linearly independent} on an interval $I$ if and only if for every $t$ in $I$, the result of one function can be represented as a scalar multiple of the other function:
    $$\bm{f(t)} = c\bm{g(t)} \text{ for all $t \in I$}$$
    This is equivalent to saying (as most textbooks present it):
    $$c_1\bm{f(t)} + c_2\bm{g(t)} = 0 \text{ for all $t \in I$}$$
    for non-zero constants $c_1$ and $c_2$
    
    Similarly, \textbf{linear dependence} is defined as when these conditions are not met, i.e.:
    $$\bm{f(t_0)} \neq c\bm{g(t)} \text{ for a $t_0 \in I$}$$
    or:
    $$c_1\bm{f(t_0)} + c_2\bm{g(t_0)} = 0 \text{ for a $t_0 \in I$}$$
    for only constants $c_1$ and $c_2$ equal to 0
    
\end{mytheo}

These definitions apply similarly in higher dimensions, linear independence of $n$ functions is the same thing as all of the $n$ functions being linearly independent to one another.

\section{Wronskian and Linear Independence}
How does the Wronskian relate to linear independence of functions?

Fascinatingly, finding the Wronskian of any two functions can tell us directly if the two functions are linearly independent:

\begin{mytheo}{Wronskian and linear independence}{wronLinInd}
    Consider two functions $f(t)$ and $g(t)$ defined as continuous on some interval $I$. If the Wronskian of these two functions is non-zero for a $t_0$ in the interval $I$, then the two functions are \textbf{linearly independence}. 
    
    $$W(f(t),g(t))(t_0) = \text{det}\left(\begin{bmatrix}
    f(t_0) & g(t_0) \\
    f'(t_0) & g'(t_0) \\
    \end{bmatrix}\right)
    \neq 0 $$
    $$\implies c_1\bm{f(t_0)} + c_2\bm{g(t_0)} = 0$$ 
    for a $t_0 \in I$ only for constants $c_1=c_2=0$ (linear independence).
\end{mytheo}
  
\hyperref[sec:prWronLinInd]{The proof can be found in the appendix}


Note that the implication is in only one way. This is important!! In general, if the Wronskian is zero for all $t \in I$, that does \textbf{not} necessarily mean that functions are linearly dependant. 
    
An example of this occurring are the functions $f(t) = t^3$ and $g(t) = |t|t^2$ over the interval $-\infty < t < \infty$. These functions are linearly independent over the interval, but the Wronskian is in fact 0 everywhere!!
\\ \\
By using basic logic principles, the contrapositive of a true implication is also true. Therefore, linearly dependent functions have a Wronskian zero everywhere in the interval in which they are defined to be linearly dependant:
    
\begin{mytheo}{Linear dependence and the Wronskian}{wronLinDep}
    Consider two functions $f(t)$ and $g(t)$ defined as continuous on some interval $I$. If the functions are \textbf{linearly dependent} for all $t \in I$, then the Wronskian is zero everywhere.
    
    $$c_1\bm{f(t)} + c_2\bm{g(t)} = 0$$ 
    for non-zero constants $c_1$ and $c_2$ (linear dependence)
    
    $$ \implies W(f(t),g(t))(t) = \text{det}\left(\begin{bmatrix}
    f(t) & g(t) \\
    f'(t) & g'(t) \\
    \end{bmatrix}\right)
    = 0 \text{, for all $t \in I$}$$
\end{mytheo}


\section{Abel's Theorem}
Let's go back to the case of second order homogeneous linear differential equations. Say we have the equation:
$$y''(t) + p_1(t)y'(t) + p_0(t)y(t) = 0$$
For any two solutions $y_1(t)$ and $y_2(t)$ which may be fundamental solutions or not, Abel's theorem yields an extremely powerful result:

\begin{mytheo}{Abel's Theorem}{abel_thm}
    Say we have a second order homogeneous linear differential equation of the form 
    $$y''(t) + p_1(t)y'(t) + p_0(t)y(t) = 0$$
    over an interval $I$ where the coefficients are continuous and the first and second derivatives of $y(t)$ are continuous and defined.
    In addition, say we have two solutions $y_1(t)$ and $y_2(t)$ that satisfy these constraints.
    
    According to Abel's theorem, the Wronskian of any two solutions will precisely be:
    
    $$W(y_1(t), y_2(t))(t) = \text{det}\left(\begin{bmatrix}
    y_1(t) & y_2(t) \\
    y_1'(t) & y_2'(t) \\
    \end{bmatrix}\right)$$
    $$= c(y_1(t),y_2(t))\exp{\left[-\int{p_1(t)dt}\right]}$$
    where $c$ is a constant that depends on $y_1(t)$ and $y_2(t)$ but not $t$.
\end{mytheo}

\hyperref[sec:abel]{The proof can be found in the appendix.}
\newline


This theorem tells us that the Wronskian of any two solutions to a second-order linear homogeneous differential equation differ only by a constant multiple.


One consequence of Abel's theorem is that if the Wronskian is non-zero at one point in the interval $I$, it is non-zero everywhere. Similarly, if the Wronskian is zero at one point, it is zero everywhere. This is because $c$ determines whether or not the Wronskian will be zero or non-zero at a point $t_0 \in I$ since the exponential term will always be non-zero. Since $c$ is universally defined in the interval (not dependent on $t$), it will be 0 or non-zero. 

This leads us to another important result:

\begin{mytheo}{Wronskian for two solutions}{wron_two_sol}
    The Wronskian for any two solutions of a second order linear homogeneous differential equation defined on some interval $I$ is either 0 everywhere on this interval or always non-zero.
    $$
    W(y_1(t), y_2(t))(t) = \text{det}\left(\begin{bmatrix}
    y_1(t) & y_2(t) \\
    y_1'(t) & y_2'(t) \\
    \end{bmatrix}\right) = 0
    $$
    for all $t \in I$
    
    OR
    
    $$
    W(y_1(t), y_2(t))(t) = \text{det}\left(\begin{bmatrix}
    y_1(t) & y_2(t) \\
    y_1'(t) & y_2'(t) \\
    \end{bmatrix}\right) \neq 0
    $$
    for all $t \in I$
\end{mytheo}

This falls directly from Abel's theorem (the constant is either 0 or non-zero on the interval $I$). 

This means that if we find the Wronskian to be 0 at any point $t_0$ in the interval, the Wronskian will be 0 everywhere on the interval. Similarly, if it is non-zero at any point, it will be non-zero everywhere. 

This is a powerful fact! This will be made even more powerful in the next section:

\section{Fundamental Solutions and Linear Dependence}
We know two major things from the previous sections about the Wronskian of two solutions to a homogeneous linear differential equation:
\begin{enumerate}
    \item \hyperref[th:unHomSol]{$W(y_1(t), y_2(t))(t_0)\neq 0 \implies $ $y_1(t)$ and $y_2(t)$ span the solution space}
    \item \hyperref[th:wronLinInd]{$W(y_1(t), y_2(t))(t_0)\neq 0 \implies $ $y_1(t)$ and $y_2(t)$ are linearly independent}
\end{enumerate}

A question that may be asked is whether linear independence and spanning the solution space of two solutions can be connected. This would be a beautiful connection to linear algebra and differential equations, where having linearly independent vectors spans the space we're considering. In fact, there is a connection through the Wronskian:


\begin{mytheo}{Wronskian and Linear Dep of solutions}{WrLinDep}
    Say we have a second order homogeneous linear differential equation of the form 
    $$y''(t) + p_1(t)y'(t) + p_0(t)y(t) = 0$$
    over an interval $I$ where the coefficients are continuous and the first and second derivatives of $y(t)$ are continuous and defined. 
    In addition say we have found two solutions $y_1(t)$ and $y_2(t)$ for this differential equation.
    
    The Wronskian of $y_1(t)$ and $y_2(t)$ will only be 0 if and only if $y_1(t)$ and $y_2(t)$ are Linearly Dependent. 
    
    In other words:
    
    $$W(y_1(t), y_2(t))(t) = 0 \implies \text{Linear Dependence}$$
    
    $$ \text{Linear Dependence} \implies W(y_1(t), y_2(t))(t) = 0$$
    
\end{mytheo}


\hyperref[sec:PrWrLinDep]{The proof of this can be found in the appendix.}

The converse is also directly implied:

\begin{mytheo}{Wronskian and Linear Ind of solutions}{WrLinIn}
    Say we have a second order homogeneous linear differential equation of the form 
    $$y''(t) + p_1(t)y'(t) + p_0(t)y(t) = 0$$
    over an interval $I$ where the coefficients are continuous and the first and second derivatives of $y(t)$ are continuous and defined. 
    In addition say we have found two solutions $y_1(t)$ and $y_2(t)$ for this differential equation.
    
    The Wronskian of $y_1(t)$ and $y_2(t)$ will only be non-zero if and only if $y_1(t)$ and $y_2(t)$ are Linearly Independent. 
    
    In other words:
    
    $$W(y_1(t), y_2(t))(t_0) \neq 0 \implies \text{Linear Independence}$$
    
   $$ \text{Linear Independence} \implies W(y_1(t), y_2(t))(t_0) \neq 0$$
    
\end{mytheo}


Since the Wronskian being non-zero at some point is equivalent to linear independence for two solutions to a linear homogeneous second-order differential equation:

\begin{mytheo}{Fundamental solutions and Linear Ind}{fund_sol}
    If two solutions to a linear homogeneous second-order differential equation $y_1(t)$ and $y_2(t)$ are linearly independent, they span the solution space for the differential equation.
\end{mytheo}

Thus, all of these facts are equivalent:

\begin{mytheo}{Equivalencies for linearly independent solutions}{EqLinInd}
    For two solutions $y_1(t)$ and $y_2(t)$ on some interval $I$ of the linear homogeneous second-order differential equation:
    $$y''(t) + p_1(t)y'(t) + p_0(t)y(t) = 0$$
    all of the following are equivalent:
    \begin{enumerate}
        \item The Wronskian of the solutions is non-zero at some point $t_0 \in I$ 
        $$W(y_1(t), y_2(t))(t_0) \neq 0 $$
        \item The Wronskian of the solutions is non-zero at \textbf{all points} $t \in I$ 
        $$W(y_1(t), y_2(t))(t) \neq 0 $$
        \item The two solutions are linearly independent:
        $$c_1y_1(t_0) + c_2y_2(t_0) = 0$$
        is only satisfied for $c_1=c_2=0$ at some $t_0 \in I$
        \item The two solutions span the entire solution space for the differential equation:
        $$\phi(t) = c_1y_1(t) + c_2y_2(t)$$
        satisfies the differential equation for all $t \in I$
    
    \end{enumerate}
    
\end{mytheo}


Similarly, the opposite is true:

\begin{mytheo}{Equivalencies for linearly dependent solutions}{EqLinDep}
    For two solutions $y_1(t)$ and $y_2(t)$ on some interval $I$ of the linear homogeneous second-order differential equation:
    $$y''(t) + p_1(t)y'(t) + p_0(t)y(t) = 0$$
    all of the following are equivalent:
    \begin{enumerate}
        \item The Wronskian of the solutions is zero at some point $t_0 \in I$ 
        $$W(y_1(t), y_2(t))(t_0) = 0 $$
        \item The Wronskian of the solutions is zero at \textbf{all points} $t \in I$ 
        $$W(y_1(t), y_2(t))(t) = 0 $$
        \item The two solutions are linearly dependent:
        $$c_1y_1(t) + c_2y_2(t) = 0$$
        is only satisfied for some non-zero $c_1$ and $c_2$ for all $t \in I$
        \item The two solutions \textbf{do not} span the entire solution space for the differential equation:
        $$\phi(t) = c_1y_1(t) + c_2y_2(t)$$
        \textbf{does not} satisfy the differential equation for all $t \in I$
    
    \end{enumerate}
    
\end{mytheo}





\chapter{Linear Differential Equation with Constant Coefficients}
\section{Introduction}

Let's zoom into the specific case of linear homogeneous differential equations with constant coefficients:

That is a differential equation in the form of:
$$y^{(n)}(t) + ... + a_1y'(t) + a_0y(t) = 0$$

In this section we will mostly consider second order differential equations, since they are significantly easier to solve, however, the results apply in general for any $n^th$ degree polynomial.

So let us look at that case:
$$y''(t) + a_1y'(t) + a_0y$$

What happens if we try a solution in the form of $y(t) = Ce^{rt}$?

When we plug it into our equation and simplify, we get (by the property of taking derivatives of exponents):

$$(Ce^{rt})'' + a_1(Ce^{rt})' + a_0Ce^{rt} = 0$$
$$(r^2cCe^{rt}+a_1rCe^{rt}+a_0Ce^{rt} = 0$$
Taking out $Ce^{rt}$, 
$$(r^2+a_1r+a_0)e^{rt} = 0$$

Since, $e^{rt}$ is always greater than 0, we can divide by both sides to get an expression for $r$:

$$r^2+a_1r+a_0 = 0$$

To recap, for the differential equation above $y(t) = Ce^{rt}$ is a solution for any $r$ that satisfies the polynomial equation above!

This holds in general:

\begin{mytheo}{A Constant Coefficients Solution}{constSol}
    For an n'th order differential equation with constant coefficients in the form:
    $$y^{(n)}(t) + ... + a_1y'(t) + a_0y(t) = 0$$
    
    $y(t) = Ce^{rt}$ will be a solution as long as it satisfies the following polynomial equation:
    
    $$r^n+\dots+a_1r+a_0=0$$
    
    The polynomial is called the \textbf{characteristic polynomial} of the differential equation above.
\end{mytheo}
The proof can be found in the appendix.

From the chapter above, we showed that in order for us to have the complete solution a linear differential equation of $n$'th degree, we need $n$ linearly independent solutions. Since $y(t) = Ce^{rt}$ is always a solution to the differential equation, it will help us find the fundamental solution. 
In the sections below, we will consider finding the complete solution to the linear differential equation with constant coefficients in the following cases for the roots of the characteristic polynomial:
\begin{enumerate}
    \item Roots are real and unique
    \item Roots are complex
    \item Some Roots are repeated
\end{enumerate}


\section{Real Unique Roots}

The following theorem is important:

\begin{mytheo}{Linear Independence of Exponentials of Different Degrees}{expLinInd}
    Consider two functions $y_1(t) = e^{r_1t}$ and $y_2(t) = e^{r_2t}$ such that $r_1 \neq r_2$. 
    \\
    \\
    The functions $y_1(t) = e^{r_1t}$ and $y_2(t) = e^{r_2t}$ has a Wronskian that is always non-zero and are linearly independent.
\end{mytheo}

\hyperref[sec:prExpLinInd]{The proof is in the appendix.}

If we have the differential equation, 
$$y^{(n)}(t) + ... + a_1y'(t) + a_0y(t) = 0$$
and the following characteristic polynomial equation:

$$r^n+\dots+a_1r+a_0=0$$

Say that we solve this polynomial equation and get $n$ real unique roots such that $r_1 \neq r_2 \neq \dots r_n$. 

Then we know that $y_1(t) = Ce^{r_1t}, y_2(t) = Ce^{r_2t}, \dots y_n(t)Ce^{r_nt}$ are all \hyperref[th:constSol]{solutions to the differential equation}. We also know that all of these solutions are \hyperref[th:expLinInd]{linearly independent since $r_1 \neq r_2 \neq \dots r_n$}. Because of these facts, we know that the following function:
$$\phi(t) = c_1e^{r_1t}+c_2e^{r_2t}+\dots c_ne^{r_nt}$$

must be the \hyperref[th:EqLinInd]{fundamental solution the differential equation since it is the combination of $n$ linearly independent solutions for a $n$'th order linear differential equation.}

Thus:

\begin{mytheo}{Complete Solution to Linear Differential Equation with Real Unique Roots}{compSolReUn}
    For a linear differential equation with constant coefficients in the form:
    $$y^{(n)}(t) + ... + a_1y'(t) + a_0y(t) = 0$$
    If we solve the characteristic polynomial:
    $$r^n+\dots+a_1r+a_0=0$$
    and get $n$ real unique roots $r_1 \neq r_2 \neq \dots r_n$, the fundamental solution is:
    
    $$\phi(t) = c_1e^{r_1t}+c_2e^{r_2t}+\dots c_ne^{r_nt}$$

\end{mytheo}


\section{Complex Roots}
\label{sec:comRoot}
If we have a polynomial of the form:
$$y^{(n)}(t) + ... + a_1y'(t) + a_0y(t) = 0$$
and instead of getting unique real roots, we get complex roots of the form $a+bi$, we could proceed as we did before and have solutions:
$$e^{(a_1+b_1i)t}, e^{(a_2+b_2i)t}, \dots, e^{(a_n+b_ni)t}$$

This would in fact be valid. However, there is a way we can take this equation and turn it into a form that is significantly easier to both visualize and easier to apply to actual physical solutions.

First, we need to introduce Euler's formula:

\begin{mytheo}{Euler's Formula}{euler}
    We can express the exponential of an imaginary number with sines and cosines using Euler's formula.
    
    $$e^{bit} = \cos(bt) + i\sin(bt)$$
\end{mytheo}

Another important theorem in algebra is the complex conjugate root theorem:

\begin{mytheo}{Complex Conjugate Root Theorem}{comConjRoot}
    For any $n$'th degree polynomial in the form:
    $$r^n+\dots+a_1r+a_0=0$$
    If the complex number $r_1=a+bi$ is a root, then another root has to be $r_2 = a-bi$. 
    In other words, if one complex number is a root of a polynomial, then it is neccessarily the case that the complex conjugate of that root is another root (complex conjugates always come in pairs).
\end{mytheo}

Both of these theorems hold true in general and are given without proof.

By the \hyperref[th:comConjRoot]{Complex Conjugate theorem}, if $r_1=a+bi$ is a solution to the characteristic polynomial $r_2 = a-bi$ must also be a solution.

Thus when $y_1(t) = e^{(a+bi)t}$ is a solution to a linear homogeneous differential equation with constant coefficients, $y_2(t) = e^{(a-bi)t}$ also must be (since they both solve the same characteristic polynomial).

Because of \hyperref[th:euler]{Euler's formula} we can represent these solutions as:
$$y_1(t) = e^{at}(\cos{bt}+i\sin{bt}$$
$$y_2(t) = e^{at}(\cos{bt}-i\sin{bt}$$

Because of superposition $y_3(t) =\frac{1}{2}y_1(t)+\frac{1}{2}y_2(t)$ is also a solution:

$$y_3(t) = \frac{1}{2}e^{at}(\cos{bt}+i\sin{bt})+ \frac{1}{2}e^{at}(\cos{bt}-i\sin{bt})$$
$$=e^{at}(\frac{1}{2}\cos{bt} + \frac{1}{2}\cos{bt})$$
$$=e^{at}\cos{bt}$$

Similarly, $y_4(t)=-\frac{1}{2}iy_1(t) + \frac{1}{2}iy_2(t)$ is also a solution:
$$y_4(t) = -\frac{1}{2}ie^{at}(\cos{bt}+i\sin{bt})  \frac{1}{2}e^{at}(\cos{bt}-i\sin{bt})$$
$$ie^{at}(-\frac{1}{2}i\sin{bt} - \frac{1}{2}i\sin{bt})$$
$$-e^{at}(i^2\sin{bt})$$
$$e^{at}(\sin{bt})$$


\label{sinLin}
These two solutions formed by taking linear combinations of the complex solutions are real and furthermore they are \hyperref[sec:prSinLin]{linearly independent!}

Thus,

\begin{mytheo}{Real Solutions from Complex roots}{realFromComplex}
    If we have a differential equation in the form 
    $$y^{(n)}(t) + ... + a_1y'(t) + a_0y(t) = 0$$
    with the characteristic polynomial:
    $$r^n+\dots+a_1r+a_0=0$$
    if we solve this polynomial and get a complex pair of conjugate roots $r_1 = a+bi$ and $r_2 = a-bi$, we can represent two \textbf{real} solutions for the original differential equation as:
    $$y_1(t) = e^{at}(\cos{bt})$$
    $$y_2(t) = e^{at}(\sin{bt})$$
\end{mytheo}

\section{Repeated Roots}
What happens when we solve our characteristic equation and get a repeated root, i.e. $r_1=r_2$ solves our equation. Then we no longer have the $n$ linearly independent solutions we need to span our solution space anywhere.

The proof of the following theorem is long, but definitely worth looking at:


\begin{mytheo}{Linearly Independent Solutions with Repeated Roots}{linIndRep}
    If we have a differential equation in the form 
    $$y^{(n)}(t) + ... + a_1y'(t) + a_0y(t) = 0$$
    with the characteristic polynomial:
    $$r^n+\dots+a_1r+a_0=0$$
    If we solve this equation and end up getting two roots $r_i=r_j$ such that there are less than $n$ equations of the form $y(t) = e^{rt}$. 
    If $r_i$ is a repeated once in the characteristic solution, then we can find two linearly independent solutions:
    $$y_i(t) = e^{r_it}$$
    $$y_j(t) = te^{r_it}$$
    
    This applies in general. If $r_i$ is repeated $n$ times, all of the following are linearly independent solutions:
    $$y_1(t) = e^{r_it}$$
    $$y_2(t) = te^{r_it}$$
    $$\vdots$$
    $$y_n(t) = t^ne^{r_it}$$
\end{mytheo}

\label{repRoot}
\hyperref[sec:PrRepRoot]{The "proof" is presented in the appendix} for the specific case of second-order differential equation. 

Note that this means that for any homogeneous differential equation with constant coefficients, we can \textbf{always} find the solutions quite easily if we know the roots of the polynomial. Finding the roots, however may end up being very difficult (especially since there is no general formula for polynomials of degree 5 and above).


\section{Application: Spring-Mass System and Damping}
\subsection{Simple Harmonic Motion}
Consider a physics example of a mass attached to a theoretically mass-less spring with a spring-constant of $k$.

[insert picture]

When we pull the spring to the right by a small $\delta x$, by Hooke's law, the restoring force to its original force is proportial to the distance stretched. The direction is in the opposite direction of stretching. Thus,
$$F = -kx$$
This is equivalent to (by Newton's 2nd Law):
$$m\dv[2]{x}{t} = -kx$$
Further simplify we get:
$$\dv[2]{x}{t} + \frac{k}{m}x = 0$$

which is a second order differential equation with constant coefficients. We can solve this by finding the roots of the characteristic polynomial:

$$r^2+\frac{k}{m} = 0$$
$$r = 0 \pm \sqrt{-\frac{k}{m}}$$

Thus we have a complex root. We know from \hyperref[sec:comRoot]{the section above} that two solutions to this are:

$$y_1(t) = e^0\cos(\sqrt{\frac{k}{m}}t) = \cos(\sqrt{\frac{k}{m}}t)$$
$$y_2(t) = e^0\sin(\sqrt{\frac{k}{m}}t) = \sin(\sqrt{\frac{k}{m}}t)$$

and that in fact any linear combination of these two solutions will also be a solution.

$$y(t) = c_1\cos(\sqrt{\frac{k}{m}}t) + c_2\sin(\sqrt{\frac{k}{m}}t)$$

The following theorem can help simplify this even more:

\begin{mytheo}{Adding sines and cosines of same frequency}{sinPlusCosin}
    The linear combination of a sine and cosine function of the same frequency is itself a cosine function with a phase shift $\phi$:
    $$a\cos(\omega t) + b\sin(\omega t) = A\cos(\omega t - \phi)$$
    where $$A = \sqrt{a^2+b^2}$$ and $$\phi = \tan^{-1}(b/a)$$
\end{mytheo}
\hyperref[sec:prSinPlusCosin]{Two proofs are given in the appendix.}

Thus, an equivalent form is:
$$y(t) = A\cos(\sqrt{\frac{k}{m}} t - \phi)$$

In both representations of the solution, the constants can be determined by the initial conditions ($c_1, c_2$ in the first case and $A, \phi$ in the second).

Thus, the solution look something like this:

\includegraphics[scale=0.4]{cos_phase.png}


\subsection{Damping}

\includegraphics[scale=0.4]{spring-mass.png}


If we introduce some external force such as friction that opposes the direction the mass is travelling and is proportional to velocity, we get another equation:

$$F = -kx(t) - bv(t)$$

Which simplifying is:

$$\dv[2]{x(t)}{t} + \frac{b}{m}\dv{x(t)}{t} + \frac{k}{m}x(t) = 0$$

This gives us the characteristic polynomial:
$$r^2 + \frac{b}{m}r + \frac{k}{m} = 0$$

To get the roots we can use the quadratic formula to get:
$$r = \frac{-\frac{b}{m} \pm \sqrt{(\frac{b}{m})^2-4\frac{k}{m}}}{2}$$
$$ = -\frac{b}{2m} \pm \frac{1}{2}\sqrt{\left(\frac{b}{m}\right)^2-4\frac{k}{m}}$$
$$ = -\frac{b}{2m} \pm \sqrt{\left(\frac{b}{2m}\right)^2-\frac{k}{m}}$$

Let's rename some constants to make solving this easier:

$$\rho = \frac{b}{2m}$$
$$\omega_n = \sqrt{\frac{k}{m}}$$
$$\omega_d = \sqrt{\rho^2 - \omega_n^2}$$

The reason we named $\sqrt{\frac{k}{m}}$ the natural frequency, is that this would be the angular frequency of the cosine function if there was no oscillation. Plugging these values in our equation:
$$r = -\rho \pm \sqrt{\rho^2-\omega_n^2}$$

Thus our complex solution will be:
$$y(t) = e^{-\rho \pm \sqrt{\rho^2-\omega_n^2}t}$$

However, using the methods we described in the section above, we can better understand what is going on when evaluating three different cases (two distinct real roots, complex roots and repeated roots)

\subsubsection{Underdamped Case}
Let's consider the case when the discriminant is less than 0 
$$\omega_d = \rho^2-\omega_n^2 < 0$$.
Thus, we should get complex roots:

$$r = -\rho \pm i\omega_d$$

We know from our \hyperref[th:realFromComplex]{previous discussion of the case where our roots are complex}, that two real solutions to our original differential equation are:

$$y_1(t) = e^{-\rho t}(\cos(\omega_d t))$$
$$y_2(t) = e^{-\rho t}(\sin(\omega_d t))$$

Thus, the complete solution is any linear combination of these two cases:
$$y(t) = c_1e^{-\rho t}(\cos(\omega_d t)) + c_2e^{-\rho t}(\sin(\omega_d t))$$
Which we can simplify to:
$$y(t) = e^{-\rho t}(c_1\cos(\omega_d t) + c_2\sin(\omega_d t))$$
$$=Ae^{-\rho t}\cos(\omega_d t - \phi)$$

Visually, this function should begin as a cosine function oscillating with an exponentially decreasing amplitude.


\subsubsection{Critically Damped Case}
Now let's consider the case where the discriminant is 0. Thus,

$$\omega_d = \rho^2-\omega_n^2 = 0$$.

Solving our characteristic equation, we get the repeated root:
$$r = -\rho$$

What should our complete solution be in this case? We can look to our \hyperref[th:linIndRep]{previous discussion on repeated roots} in the characteristic polynomial to see that two linearly independent solutions should be:


$$y_1(t) = e^{-\rho t}$$
$$y_2(t) = te^{-\rho t}$$

Thus our complete solution is any linear combination of these solutions:

$$y = c_1e^{-\rho t} + c_2te^{-\rho t} $$


This will tend to 0 fairly quickly because of the two exponentials.


\subsubsection{Overdamped Case}
Let's consider the case when the discriminant is greater than 0 
$$\omega_d = \rho^2-\omega_n^2 > 0$$.

Since the discriminant is positive, we get two distinct real solutions:
$$r_1 = -\rho + \omega_d$$
$$r_2 = -\rho - \omega_d$$

Thus our two linearly independent solutions will be:
$$y_1(t) = e^{-(\rho - \omega_d)t}$$
$$y_2(t) = e^{-(\rho + \omega_d)t}$$

We know that $\rho > \omega$ so both of these functions exhibit exponential decay. Thus, the complete solution should also exhibit exponential decay:

$$y(t) = c_1e^{-(\rho - \omega_d)t} + c_2e^{-(\rho + \omega_d)t}$$

\subsection{Summary}
For a spring-mass system we have 4 different cases:
\newline


\begin{tabular}{ c c c c}
 \textbf{Situation} & \textbf{Case} & \textbf{Roots} & \textbf{Solution} \\
 \hline
 No Damping & $b=0$ & Complex Roots $\pm i\omega_n$ & $A\cos(\omega_nt - \phi)$ \\ 
 Underdamped & $b^2 < 4mk$ & Complex Roots $-\rho \pm i\omega_n$ &  $Ae^{-\rho t}\cos(\omega_d t - \phi)$\\  
 Critically Damped & $b^2 = 4mk$ & Repeated Root $-\rho$ & $(c_1 + c_2t)e^{-\rho t}$  \\
 Overdamped & $b^2 > 4mk$ & Real Roots $-\rho \pm \omega_d$  & $c_1e^{-(\rho - \omega_d)t} + c_2e^{-(\rho + \omega_d)t}$ \\
\end{tabular}


where:

$$\rho = \frac{b}{2m}$$
$$\omega_n = \sqrt{\frac{k}{m}}$$
$$\omega_d = \sqrt{\rho^2 - \omega_n^2}$$

This is easier to understand visually:

\includegraphics[scale=0.5]{damping.png}


\chapter{Solving Non-homogeneous Linear Differential Equations}
\section{Introduction}
In the previous sections, we mostly focused on the homogeneous case of linear differential equations. In this case we will focus on non-homogeneous equations, that is, there is a term in the differential equation that is only dependent on the independent variable. Thus, we will be considering equations in the form:


$$y''(t) + p_1(t)y'(t) + p_0(t)y(t) = g(t)$$

where $y(t)$ is twice differentiable on an interval $I$ and $p_1(t), p_0(t) and g(t)$ are defined and continuous in this interval.

You can imagine $g(t)$ to be some external force that doesn't depend on our original system (only on time). We will use this fact to study the effect of putting an external force on the spring-mass-dashpot system we talked about before.

\section{Finding the Non-Homogeneous Solution}

The following theorem gives us the method of finding a non-homogeneous solution for a differential equation:

\begin{mytheo}{Non-homogeneous Linear Solution}{nhSol}
 For a non-homogeneous solution of the form
 $$y^{(n)}(t) + ... + p_1(t)y'(t) + p_0(t)y(t) = g(t)$$
 where the coefficients are continuous over an interval $I$, the solution is of the form
 $$\phi(t) = y_h(t) + y_p(t)$$
 where $y_h(t)$ is the complete solution to homogeneous differential equation:
 $$y^{(n)}(t) + ... + p_1(t)y'(t) + p_0(t)y(t) = 0$$
 and $y_p(t)$ is just any particular solution to the original non-homogeneous differential equation.
\end{mytheo}

\hyperref[sec:prNhSol]{The proof is given in the appendix}

This theorem is saying that in order for us to find the solution to a non-homogeneous differential equation we simply need to:
\begin{enumerate}
    \item Find the solutions to the homogeneous case
    \item Find just one function that satisfies our initial non-homogeneous differential equation
    \item Add these solutions together
\end{enumerate}


\section{Non-homogeneous Equation with Constant Coefficients and Exponential Response}

Consider the case we did in the chapter above, a differential equation with constant coefficients. However, this time, let's add a an exponential term only dependant on $t$, so our equation will look like:

$$y^{(n)}(t) + ... + a_1y'(t) + a_0y(t) = be^{\omega t}$$

where $b$ is just a constant value.

As we discussed in the section above, the complete solution to this differential equation is the complete set of all homogeneous solutions plus one particular solution:
$$y(t) = y_h(t) + y_p(t)$$

Using the methods we discussed in the previous chapter, we can solve for the homogeneous solution by finding the roots of the characteristic polynomial for the homogeneous differential equation:

$$y^{(n)}(t) + \dots + a_1y'(t) + a_0y(t) = 0$$

However, how do we go about finding one particular solution?

What happens if we try plugging in $ce^{\omega t}$ as our solution, where $c$ is just an arbritrary constant. Thus we should get:

$$(ce^{\omega t})^{(n)} + \dots + a_1(ce^{\omega t})' + a_0(ce^{\omega t}) = be^{\omega t}$$

Using the fact that the derivative of an exponent is itself and the linearity of the derivative operator, we see that:
$$(\omega^n + \dots + a_1\omega + a_0)ce^{\omega t} = be^{\omega t}$$

We know that $e^{\omega t}$ is never 0 so we can divide both sides by it:
$$(\omega^n + \dots + a_1\omega + a_0)c = b$$

If we suppose that $\omega^n + \dots + a_1\omega + a_0 \neq 0$ (which is in fact a big assumption), we see that we can solve for c:

$$c = \frac{b}{\omega^n + \dots + a_1\omega + a_0}$$

Thus, we have found a particular solution!:

$$y_p(t) =  \frac{b}{\omega^n + \dots + a_1\omega + a_0}e^{\omega t}$$

If we represent the characteristic polynomial as a function:

$$P(r) = r^n + \dots + a_1r + a_0$$

We see that the denominator of our solution is just the characteristic polynomial evaluated at $\omega$.

Thus more compactly, the particular solution is:

$$y_p(t) = \frac{b}{P(\omega)} e^{\omega t}$$
if $P(\omega) \neq 0$



Thus:

\begin{mytheo}{Exponential Response Formula}{ERF}
    For a non-homogeneous differential equation in the form:
    $$y^{(n)}(t) + \dots + a_1y'(t) + a_0y(t) = be^{\omega t}$$
    and the characteristic polynomial
    $$P(r) = r^n + \dots + a_1r + a_0$$
    
    A particular solution to this differential equation is:
    
    $$y_p(t) = \frac{b}{P(\omega)} e^{\omega t}$$
    if $P(\omega) \neq 0$
    
 
\end{mytheo}

What do we do in the specific case of a $\omega_0$ where $P(\omega_0) = 0$? We can evaluate the limit as we approach the value the particular solution

$$y_p(t) = \lim_{\omega \to \omega_0}\frac{b}{P(\omega)} e^{\omega t}$$

We can use L'Hopital's Rule to help evaluate this limit when it is in fact indeterminate $P(\omega_0) = 0$.

L'Hopital's Rule (not stated formally) says that if the function composed of the division of two other functions is indeterminant at some point $x_0$ we can evaluate the limit at the derivatives of both of these functions instead provided the functions are differentiable.

Thus,
if $P(\omega_0) = 0$, but $P'(\omega_0) \neq 0$, then:

$$y_p(t) = \lim_{\omega \to \omega_0}\frac{b}{P(\omega)} e^{\omega t} = \frac{b}{P'(\omega)} te^{\omega t}$$

If $P'(\omega_0) = 0$ also, but the second derivative is non-zero then:

$$y_p(t) = \lim_{\omega \to \omega_0}\frac{b}{P(\omega)} e^{\omega t} = \frac{b}{P''(\omega)} t^2e^{\omega t}$$


and so on.

Thus:

\begin{mytheo}{Generalized Exponential Response Formula}{GERF}
    For a non-homogeneous differential equation in the form:
    $$y^{(n)}(t) + \dots + a_1y'(t) + a_0y(t) = be^{\omega t}$$
    and the characteristic polynomial
    $$P(r) = r^n + \dots + a_1r + a_0$$
    
    At the first non-zero value of $P(r)$ evaluated at $\omega$ (say at the m'th derivative), $P^{m}(\omega) \neq 0$ then we can always find a particular solution by:
    
    $$y_p(t) = \frac{b}{P^{(m)}(\omega)} t^me^{\omega t}$$
    if $P^{(m)}(\omega) \neq 0$
    and all the lower derivatives $P(\omega), P'(\omega), \dots, P^{(m-1)} = 0$
    
 
\end{mytheo}

\section{Complexification for sines and cosines}

The method above only works for exponentials. However, by using Euler's formula:

$$\cos(\omega t) + i\sin(\omega t) = e^{i\omega t}$$

we can use the same method for sinusoidal functions.

Imagine that we have the differential equation:

$$y^{(n)}(t) + \dots + a_1y'(t) + a_0y(t) = b\cos(\omega t)$$

Say that we make a new function such that:
$$z(t) = y(t) + iq(t) $$

In other words: $y(t) = \Re[z(t)]$

Thus, plugging it into the LHS of the equation we get:

$$z(t)^{(n)}(t) + \dots + a_1z'(t) + a_0z(t)$$
$$=(y(t) + iq(t))^{(n)} + \dots + a_1(y(t) + iq(t))' + a_0(y(t) + iq(t))$$

which by linearity gives us:

$$y^{(n)}(t) + \dots + a_1y'(t) + a_0y(t) + i(q^{(n)}(t) + \dots + a_1q'(t) + a_0q(t))$$

Hmm... The first part of this expression looks a lot like the differential equation we originally were trying to solve and if we plug in our original value for it, we get:

$$b\cos(\omega t) + i(q^{(n)}(t) + \dots + a_1q'(t) + a_0q(t))$$

Remember we arbitrarily introduce the function $q(t)$ so we can set the value of it to whatever we want. Say we set $q^{(n)}(t) + \dots + a_1q'(t) + a_0q(t))$ to $\sin(\omega t)$:

We get:

$$b(\cos(\omega t) + i\sin(\omega t))$$

which by Euler's formula is:

$$be^{i\omega t}$$

Thus,
$$z^{(n)}(t) + \dots + a_1z'(t) + a_0z(t) = be^{i\omega t}$$

where $y(t) = \Re[z(t)]$

We know how to solve for $z_p(t)$ using the methods we discussed above! We just need to get the real part of it to solve for our original differential equation.

We can apply the same procedure for a differential equation where instead of cosine, we had sine. We would just solve the complexified equation and take the imaginary part of the solution.

This gives us the method of complexification:

\begin{mytheo}{Complexification}{complexification}
    For a non-homogeneous differential equation with constant coefficien569+ts, if we have a sinusoidal input in the form:
    $$y^{(n)}(t) + \dots + a_1y'(t) + a_0y(t) = b\cos(\omega t)$$
    or 
    $$y^{(n)}(t) + \dots + a_1y'(t) + a_0y(t) = b\sin(\omega t)$$
    We can find a particular solution for these cases if we solve a different differential equation:
    $$z^{(n)}(t) + \dots + a_1z'(t) + a_0z(t) = be^{i\omega t}$$
    which we can solve using the \hyperref[th:GERF]{Generalized ERF.}
    
    To find the solutions for our differential equation we take Real or Imaginary part of the solution $z_p(t)$. Thus in the first case with cosine:
    $$y_p(t) = \Re{z_p(t)}$$
    In the second case with sine:
    $$y_p(t) = \Im{z_p(t)}$$
\end{mytheo}

\section{Constant Coefficients with a Polynomial Response}
Let's say that we're given the differential equation

$$y^{(n)}(t) + \dots + c_1y'(t) + c_0y(t) = a_0t^n + a_1t^{n-1} + \dots a_n$$

(Notice that I switched $a_0 \dots a_n$ with $c_n \dots  c_0$. I did this because of convention in a lot of textbooks)

It turns out that we can find the solution for this case:

\begin{mytheo}{Polynomial Response}{polyResp}
    Given a non-homogeneous differential equation with constant coefficients of the form:
    $$y^{(n)}(t) + \dots + c_1y'(t) + c_0y(t) = a_0t^n + a_1t^{n-1} + \dots a_n$$
    
    A particular solution will take the form:
    
    $$y_p(t) = A_0t^n + \dots + A_n$$
    if $c_0 \neq 0$:
    
    If $c_0=0$, then the form will be:
    
    $$y_p(t) = t(A_0t^n + \dots + A_n)$$
    if $c_1 \neq 0$:
    
    If both $c_0=c_1=0$, then the particular solution will be:
    $$y_p(t) = t^2(A_0t^n + \dots + A_n)$$
    if $c_2 \neq 0$:

    This process continues on. Thus when we have $c_0 = c_1 = \dots = c_s = 0$, the form of a particular solution is:
    $$y_p(t) = t^{s+1}(A_0t^n + \dots + A_n)$$
    if $c_{s+1} \neq 0$:
    
\end{mytheo}

\hyperref[sec:prPolyResp]{The proof of this is given in the appendix.}


After we know the form of a particular solution, it is relatively straightforward to go about finding the actual coefficients. We can just plug it into our equation and relate like terms together.

\section{Method of Undetermined Coefficients}
We might run into a specific case of a differential equation where the response is in fact a sum or product of an exponential and polynomial. How do we deal with those cases to find a particular solution?

Let's start with sums:

This theorem applies in general (doesn't require terms to be polynomials or exponentials):

\begin{mytheo}{Particular Solution to Sum of different responses}{sumResp}
    Given a differential equation in the form of:
    $$y^{(n)}(t) + \dots + c_1y'(t) + c_0y(t) = g_1(t) + \dots + g_m(t)$$
    and given that we have a particular solution for the differential equations:
    $$y^{(n)}(t) + \dots + c_1y'(t) + c_0y(t) = g_1(t)$$
    $$\vdots$$
    $$y^{(n)}(t) + \dots + c_1y'(t) + c_0y(t) = g_m(t)$$
    which we will denote as $y_1(t), \dots, y_m(t)$ respectively.
    
    A particular solution to the original differential equation can be found by taking the sum of these particular solutions:
    $$y_p(t) = y_1(t) + \dots + y_m(t)$$
    
\end{mytheo}

The proof of this fact is relatively simple and relies on the linearity of the problem (almost exactly the same proof as for homogeneous differential equations).


\section{Method of Variation of Parameters}
The method of variation of parameters allows us a method for determining the solution to an non-homogeneous n'th order linear differential equation, provided we know all of the homogeneous solutions.

\begin{mytheo}{Variation of Parameters}{var}
    Say we have a differential equation of the form:

    $$y^{(n)}(t) + \dots + p_1(t)y'(t) + p_0(t)y(t) = g(t)$$

    Say that the n solutions to the homogeneous are:
    $y_1(t), y_2(t), \dots, y_n(t)$ respectively.
    
    We can always find a particular solution to the non-homogeneous differential equation above by:
    
    $$y_p(t) = u_1(t)y_1(t) + u_2(t)y_2(t) + \dots + u_n(t)y_n(t)$$
    
    where the functions $u_1(t), \dots, u_n(t)$ can be determined by computing the derivatives by:
    
    
$$
\begin{bmatrix}
    u_1'(t) \\
    u_2'(t) \\
    \vdots \\ 
    u_n'(t)
\end{bmatrix}
=
\begin{bmatrix}
    y_1(t) & y_2(t) & \dots  & y_n(t) \\
    y_1'(t) & y_2'(t) & \dots  & y_n'(t) \\
    \vdots & \vdots & \ddots & \vdots \\
    y_1^{(n)}(t) & y_2^{(n)}(t) & \dots  & y_n^{(n)}(t) \\
\end{bmatrix}^{-1}
\begin{bmatrix}
    0 \\
    0 \\
    \vdots \\ 
    g(t)
\end{bmatrix}
$$
Once we have the derivatives, we can integrate to find the actual functions $u_1(t), \dots, u_n(t)$
\end{mytheo}

\hyperref[sec:prVar]{The (lengthy) proof is in the appendix.}

This gives us a general way to solve for a particular solution of any non-homogeneous linear differential equation given we know the homogeneous solutions! 

\section{Application: Resonance}
Imagine a spring-mass system we discussed earlier without damping. What if we applied a sinusoidal force to our oscillator?

Thus,
$$F = -kx + A\cos(\omega t)$$
$$\dv[2]{x}{t} + \frac{k}{m}x = A\cos(\omega t)$$

This is a non-homogeneous differential equation. If we use the ERF to solve for a particular solution, we get:

$$x_p(t) = \frac{A\cos(\omega t)}{(k/m)^2-\omega^2}$$

if $\frac{k}{m} \neq \omega$

However, something interesting happens as $\omega$ is in fact the natural frequency $\frac{k}{m}$. We can see that as it approaches this value, we get higher and higher values for x. We can use l'Hopital's rule to determine the exact value of this limit which is:

$$\lim_{\omega \to k/m}(\frac{A\cos(\omega t)}{(k/m)^2-\omega^2}) = \frac{At\sin(\omega t)}{2\omega}$$


Notice that now the response is in fact a sinusoidal function with increasing amplitude!

This phenomenon is called \textbf{pure resonance}





\chapter{Linear Systems of Differential Equations}
\section{Introduction}
Linear systems of differential equations become important when we want to model rates of change that depend on multiple variables and those variables further depend on other variables.

For example (and oversimplifying greatly), the rate of change in the population of wolfs in a forest may depend on the current population of wolfs and the population of their prey (such as deer). The rate of change in the deer population also depends on the population of predators and the current population of deer. Therefore we could model the system as:

$$x' = ax + by$$
$$y' = cx + dy$$

This can also be thought of in matrix form:

$$
\begin{bmatrix}
    x' \\
    y' \\
\end{bmatrix}
=
\begin{bmatrix}
    a & b \\
    c & d \\
\end{bmatrix}
\begin{bmatrix}
    x \\
    y \\
\end{bmatrix}
$$

We will discuss how to solve such matrices and understand the long term behaviour of systems like these.

In this chapter we will use the convention that for each variable $x_i$, we will not put $(t)$ afterward to represent a function (even though it is in fact a function).

\section{Existence of Solutions}
In this section, we will consider the system of differential equation as follows:

$$x_1' = F_1(x_1, \dots, x_n)$$
$$\vdots$$
$$x_n' = F_2(x_1, \dots, x_n)$$

We define a \textbf{solution} to this differential equation when we have:

$$x_1 = \phi_1(t)$$
$$\vdots$$
$$x_n = \phi_n(t)$$

defined on some interval $I$ where the solutions are differential and satisfy the initial system of equations.

There is an Existence and Uniqueness Theorem for systems of differential equations as well:

\begin{mytheo}{Existence and Uniqueness Theorem for System}{exUnSys}
    For the system of differential equation:
    $$x_1' = F_1(x_1, \dots, x_n)$$
    $$\vdots$$
    $$x_n' = F_2(x_1, \dots, x_n)$$
    
    If $F_1, \dots F_n$  and all of the partial derivatives of $F_1, \dots F_n$ with respect to $x_1, \dots x_n$ ($n^2$ total partial derivatives) are continuous on some region $R$ of $t, x_1, \dots, x_n$. 
    
    Furthermore say that at some point in R, say
    $t_0, x_1^0, \dots x_n^0$, some initial constraint is satisfied:
    $$x_1(t_0) = x_1^0$$
    $$\vdots$$
    $$x_n(t_0) = x_n^0$$
    
    Then there must exist a unique solution on some interval I for which $x_1 = \phi_1(t), \dots, x_n = \phi_n(t)$ such that both the initial system and the initial conditions are satisfied.
    
\end{mytheo}

The proof of this theorem is not given.

Note one thing: the partial derivatives of $F_1, \dots, F_n$ with respect to $t$ are irrelevant in deciding if we can find a solution or not.


Just like in the case of $n'th$ order differential equations, there are both linear can non-linear cases.

A \textbf{linear} system of differential equations is one where each function $F_1, \dots, F_n$ are combinations of $x_1, \dots, x_n$ such that the coefficients only depend on time. Thus a linear system of equations looks like:


$$x_1' = p_{11}(t)x_1 + \dots + p_{1n}(t)x_n +g_1(t)$$
$$\vdots$$
$$x_n' = p_{n1}(t)x_1 + p_{nn}(t)x_n + g_n(t)$$

We will be dropping the $(t)$ for the functions $p_11(t), \dots, p_nn(t)$ and $g_1(t), \dots, g_n(t)$ in future discussions.

Another way of viewing linear systems is that we can represent them in matrix form:


$$
\begin{bmatrix}
    x_1' \\
    \vdots \\
    x_n' \\
\end{bmatrix}
=
\begin{bmatrix}
    p_{11} &  \dots & p_{1n} \\
    \vdots & \ddots & \vdots \\
    p_{n1} &  \dots & p_{nn} \\
\end{bmatrix}
\begin{bmatrix}
    x_1 \\
    \vdots \\
    x_n \\
\end{bmatrix}
+
\begin{bmatrix}
    g_1 \\
    \vdots \\
    g_n \\
\end{bmatrix}
$$

which is also:

$$\mathbf{x}' = \mathbf{Px} + \mathbf{g}$$

where bold letters represent matrices

If $\mathbf{g} = \mathbf{0}$, we call it a \textbf{homogeneous} system of differential equation and if it is non-zero it is \textbf{non-homogeneous}.


If $p_{11}, \dots, p_{nn}$ and $g_1, \dots, g_n$ are continuous, then there exists a unique solution for some initial conditions.

Why?
Since the partial derivatives of all of the $F_1, \dots, F_n$ of a linear system of differential equation are in fact $p_11, \dots p_nn$, we can just check the continuity of these functions to satsisfy the Existence and Uniqueness Theorem.



Let's consider the case of a linear, homogeneous system:


$$\mathbf{x}' = \mathbf{Px}$$


(Note on convention:
We will denote solutions by $\mathbf{x}^{(1)}, \dots \mathbf{x}^{(n)}$. In addition, the solution vectors components will be labelled as:

$$\mathbf{x}^{(j)} = 
\begin{bmatrix}
    x_1^{(j)} \\
    \vdots \\
    x_n^{(j)} \\
\end{bmatrix}
$$
)

The following theorem is very similar to that of superposition for $n$'th order differential equations:

\begin{mytheo}{Sum of Two Homogeneous Solutions}{sysSum}
    If $\mathbf{x}^{(1)}$ and $\mathbf{x}^{(2)}$ are both solutions to the some homogeneous system in the form of
    $$\mathbf{x}' = \mathbf{Px}$$
    than any linear combination of these two solutions is also a solution.
    $$\mathbf{x}^{(3)} = c_1\mathbf{x}^{(1)} + c_2\mathbf{x}^{(2)}$$
\end{mytheo}


This applies again, just like the $n$'th order case because taking derivatives is a linear operator.

This begs the question we had in the $n$'th order case, how do we know we have enough solutions to span the solution space?

Just like before, we know that we have enough solution vectors $\mathbf{x}^{(1)}, \dots, \mathbf{x}^{(n)}$ if and only if for any $n$ set of initial conditions $x_1(t_0) = x_1^0, \dots, x_n(t_0) = x_n^0$ we can find a unique solution.

By the principle of superposition, we know that any solution can be composed of a linear combination of other solutions.

Imagine if we could find some solutions $\mathbf{x}^{(1)}. \dots, \mathbf{x}^{(n)}$ such that we could always find a linear combination such that we solve for the unique solution for \textbf{any} initial value problem:

$$
\begin{bmatrix}
    \vert & & \vert \\
    \mathbf{x}^{(1)}(t_0) & \dots & \mathbf{x}^{(n)}(t_0) \\
    \vert & & \vert \\
\end{bmatrix}
\begin{bmatrix}
    c_1 \\
    \vdots \\
    c_n \\
\end{bmatrix}
=
\begin{bmatrix}
    x_1^0 \\
    \vdots \\
    x_n^0 \\
\end{bmatrix}
$$

If $\mathbf{x}^{(1)}. \dots, \mathbf{x}^{(n)}$ are in fact our basis solutions than we should be able to find a unique $c_1, \dots, c_n$ that solve this system:

$$
\begin{bmatrix}
    c_1 \\
    \vdots \\
    c_n \\
\end{bmatrix}
=
\begin{bmatrix}
    \vert & & \vert \\
    \mathbf{x}^{(1)}(t_0) & \dots & \mathbf{x}^{(n)}(t_0) \\
    \vert & & \vert \\
\end{bmatrix}^{-1}
\begin{bmatrix}
    x_1^0 \\
    \vdots \\
    x_n^0 \\
\end{bmatrix}
$$


Let's put some names to things:

$$\mathbf{X(t)} = 
\begin{bmatrix}
    \vert & & \vert \\
    \mathbf{x}^{(1)}(t) & \dots & \mathbf{x}^{(n)}(t) \\
    \vert & & \vert \\
\end{bmatrix}
$$

$$
\mathbf{c} = 
\begin{bmatrix}
    c_1 \\
    \vdots \\
    c_n \\
\end{bmatrix}
$$

$$
\mathbf{x^0} =
\begin{bmatrix}
    x_1^0 \\
    \vdots \\
    x_n^0 \\
\end{bmatrix}
$$

Therefore, the following theorem must hold:


\begin{mytheo}{Fundamental Set of Solutions}{fundSys}
For a system of linear differential equations:

$$\mathbf{x'} = \mathbf{Px}$$

If we solve for $n$ solutions 
$\mathbf{x}^{(1)}, \dots, \mathbf{x}^{(n)}$ such that:

$$\mathbf{X(t)} = 
\text{det}\left(\begin{bmatrix}
    \vert & & \vert \\
    \mathbf{x}^{(1)}(t_0) & \dots & \mathbf{x}^{(n)}(_0t) \\
    \vert & & \vert \\
\end{bmatrix}\right)
\neq 0
$$

then the solutions $\mathbf{x}^{(1)}, \dots, \mathbf{x}^{(n)}$ form \textbf{fundamental set of solutions} such that we can we can solve for any unique solution to an IVP by taking a linear combination of these solutions. In addition, the matrix formed by taking the columns of the $n$ solutions is called the \textbf{Wronskian}.

\end{mytheo}

Notice how the Wronskian appears again! This time, instead of using the matrix formed by taking the derivatives of a set of solutions over again, it is formed by just putting the solutions as column vectors.

Another interesting parallel to the $n'th$ order differential equation case is:

\begin{mytheo}{Wronskian General Form}{wronAbelSys}
    For a system of differential equations defined in some interval $I$ in the form:
    $$\mathbf{x'} = \mathbf{Px}$$
    
    For any $n$ solutions $\mathbf{x}^{(1)}, \dots, \mathbf{x}^{(n)}$
    
    the Wronskian will satisfy the following differential equation:
    
    $$\dv{W}{t} = (p_11 + \dots + p_nn)W$$
    
    Solving this gives us the general form of the Wronskian:
    
    $$W = c\exp{\int{p_{11} + \dots + p_{nn})dt}}$$
\end{mytheo}
This is a parallel to Abel's Theorem!!

\hyperref[sec:wronAbelSys]{The proof of this for a system of two equations is given in the appendix.}

This directly implies the following:

\begin{mytheo}{Wronskian over interval for solutions}{wronOverI}
    For any set of solutions $\mathbf{x}^{(1)}, \dots, \mathbf{x}^{(n)}$, the Wronskian is either always 0 or never 0.
\end{mytheo}


















\section{Homogeneous Solutions of a System of Differential Equation}
How do we go about finding the solutions to a homogeneous system of differential equations in the form of a system:

$$\mathbf{x'} = \mathbf{Px}$$

Since we see the expression $x'$ on the left and $x$ on the right, we might guess that this has something vaguely to do with exponentials.

Let's try guessing something of the form:

$$\mathbf{x} = \mathbf{v}e^{\lambda t}$$

where $\mathbf{v}$ is just some vector.

Plugging this in, we get:

$$(\mathbf{v}e^{\lambda t})' = \mathbf{Pv}e^{\lambda t}$$

Since $\mathbf{v}$ is just a constant, we can simplify this to:

$$\mathbf{v}\lambda e^{\lambda t} =  \mathbf{Pv}e^{\lambda t}$$

Since $e^{\lambda t}$ is never 0, we can divide both sides by it:

$$\mathbf{v}\lambda =  \mathbf{Pv}$$

Rearranging terms, we have:

$$\mathbf{Pv} = \lambda\mathbf{v}$$

From linear algebra, we know that this can be solved by the eigenvectors and eigenvalues of $\mathbf{P}$!

Thus, our solutions are constrained by values:

$$\text{det}(\mathbf{P} - \lambda\mathbf{I}) = 0$$

Taking the determinant of the expression above gives us some polynomial of $n$'th degree in $\lambda$, and thus we have 3 different cases:

\begin{enumerate}
    \item Eigenvalues are are all real and unique.
    \item Eigenvalues are in complex conjugate pairs
    \item Eigenvalues are repeated
\end{enumerate}

\subsection{Real and Unique Eigenvalues}
If all the eigenvalues are real and unique, then we have $n$ linearly independent eigenvectors associated with each eigenvalue. Therefore, we can find $n$ linearly independent solutions:


$$ \mathbf{x_1}(t)=\mathbf{v_1}e^{\lambda_1 t}$$
$$\vdots$$
$$ \mathbf{x_n}(t)=\mathbf{v_n}e^{\lambda_n t}$$

We can put these solutions as columns and take the determinant to verify that the Wronskian is in fact non-zero everywhere.

Thus, we have a fundamental set of solutions!!

Our general solution will be in the form:

$$ \mathbf{x}(t) = c_1\mathbf{v_1}e^{\lambda_1 t} + \dots + \mathbf{v_n}e^{\lambda_n t}$$



\subsection{Complex Conjugate Pairs of Eigenvalues}
Suppose we get complex eigenvalues for the following system:

$$(\mathbf{P} - \lambda\mathbf{I})\mathbf{v} = \mathbf{0}$$

Let's take the complex conjugate of both sides:

$$\overline{(\mathbf{P} - \lambda\mathbf{I})\mathbf{v}} = \overline{\mathbf{0}}$$

If $\mathbf{P}$ is real, this simplifies to:

$$(\mathbf{P} - \overline{\lambda}\mathbf{I})\overline{\mathbf{v}}= \mathbf{0}$$

which is another eigenvalue, eigenvector pair. Thus, the following holds:

\begin{mytheo}{Complex Eigenvectors and Eigenvalues come in pairs}{compEigen}
    If the eigenvalue of some matrix is complex, $\lambda_1 = a + ib$, for some real $a,b$ and the eigenvector associated with this eigenvector is $\mathbf{v_1}$, then there we can always find a second eigenvalue, eigenvector combination 
    $$\lambda_2 = \overline{\lambda_1} = a - ib$$
    $$\mathbf{v_2} = \overline{\mathbf{v_1}}$$
\end{mytheo}


Thus, if we have the solution:

$$\mathbf{x_1}(t)=\mathbf{v_1}e^{(a+bi) t}$$
then another solution is formed by:
$$\mathbf{x_2}(t)=\overline{\mathbf{v_1}}e^{(a-bi) t}$$

We also know that the real and imaginary part of a number is:

$$\Re{x} = \frac{x+\overline{x}}{2}$$
$$\Im{x} = \frac{x-\overline{x}}{2i}$$

In our case:
$$\Re{\mathbf{x_1}(t)} = \frac{\mathbf{x_1}(t)+\overline{\mathbf{x_1}(t)}}{2}=\frac{\mathbf{x_1}(t)+\mathbf{x_2}(t)}{2}$$

and:
$$\Im{\mathbf{x_1}(t)} = \frac{\mathbf{x_1}(t)-\overline{\mathbf{x_1}(t)}}{2i}=\frac{\mathbf{x_1}(t)-\mathbf{x_2}(t)}{2i}$$

Notice that since the real and imaginary parts of the solution of $\mathbf{x_1}(t)$ are in fact linear combinations of two solutions, they are in fact solutions themselves!! They also end up being real solutions.

Thus:
\begin{mytheo}{Solution to Complex Eigenvalues}{compEigenvals}
    Say we are solving the equation:
    $$\mathbf{x'} = \mathbf{Px}$$
    and we plug in a solution of the form:
    $$\mathbf{x} = \mathbf{v}e^{\lambda t}$$
    and solve for the corresponding eigenvalue equation and get a complex conjugate pair for the eigenvalues and eigenvectors.
    
    We can find two solutions to this complex pair by taking the real and imaginary part of one solution formed by the complex eigenvalue-eigenvector pair:
    
    $$\mathbf{x_1(t)} = \Re{\mathbf{v_1}e^{\lambda_1 t}}$$
    $$\mathbf{x_2(t)} = \Im{\mathbf{v_1}e^{\lambda_1 t}}$$
    
    where both $\lambda_1$ and $\mathbf{v_1}$ are complex
\end{mytheo}

\subsection{Repeated Eigenvalues}
\label{sec:repEigen}
What happens when we get some repeated eigenvalues?

Well, there are two situations that can arise. One is where we still can get out linearly independent eigenvectors out of the same repeated eigenvalue.

So for example, if we have the matrix $\lambda\mathbf{I}$:
$$
\begin{bmatrix}
    \lambda & 0  \\
    0 & \lambda  \\
\end{bmatrix}
$$

There is just one eigenvalue $\lambda$. However, there are two linearly indepedent eigenvectors that can be formed 

$$
\mathbf{v_1}
=
\begin{bmatrix}
    1 \\
    0 \\
\end{bmatrix},
\mathbf{v_2}
=
\begin{bmatrix}
    0 \\
    1 \\
=\end{bmatrix},
$$

Thus, in this case, we can proceed as before as we did in the real and unique eigenvalue case.

However, what if we get a matrix like:

$$
\begin{bmatrix}
    1 & 1  \\
    0 & 1  \\
\end{bmatrix}
$$

In this case, there is only one eigenvalue, $\lambda = 1$. However, we \textbf{cannot} extract multiple linearly independent eigenvectors. There is only one eigenvector in this case:

$$
\mathbf{v_1}
=
\begin{bmatrix}
    1 \\
    0 \\
\end{bmatrix},
$$

What should we do in this case to find another linearly independent solution?


Say we have the system:

$$\mathbf{x'} = \mathbf{Px}$$

Say that we extracted one eigenvalue and one non-zero eigenvector, but we're trying to find another solution. We know for sure, that one solution can be given by:

$$\mathbf{x_1}(t) = \mathbf{v_1}e^{\lambda_1 t}$$

For the second solution let's try just multiplying by $t$:

$$\mathbf{x_2}(t) \stackrel{?}{=} \mathbf{v_1}te^{\lambda_1 t}$$


Plugging this into our original system, we get:

$$(\mathbf{v_1}te^{\lambda_1 t})' = \mathbf{P}\mathbf{v_1}te^{\lambda_1 t}$$

Using the chain rule and eigenvector properties, we get:

$$\mathbf{v_1}e^{\lambda_1 t} + \lambda_1\mathbf{v_1}te^{\lambda_1 t} = \lambda_1 \mathbf{v_1}te^{\lambda_1 t}$$

which gives us:
$$\mathbf{v_1}e^{\lambda_1 t} = 0$$
which after simplifying more gives us:
$$\mathbf{v_1} = 0$$

This is contradiction, since we assumed $\mathbf{v_1}$ was non-zero to begin with! Thus, our guess was incorrect :(


Hmm..., looking at how the "solution" failed above, we see that on the LHS that there is both a $te^{\lambda_1 t}$ and $e^{\lambda_1 t}$ component. Thus, let's try to guess:


$$\mathbf{x_2}(t) \stackrel{?}{=} \mathbf{v_1}te^{\lambda_1 t} + \mathbf{v_2}e^{\lambda_1 t}$$

where $\mathbf{v_2}$ is an arbitrary vector.

Plugging this into our original system, we get:


$$(\mathbf{v_1}te^{\lambda_1 t} + \mathbf{v_2}e^{\lambda_1 t})' = \mathbf{P}(\mathbf{v_1}te^{\lambda_1 t} + \mathbf{v_2}e^{\lambda_1 t})$$

Simplifying this expression using the chain rule:

$$\mathbf{v_1}e^{\lambda_1 t} + \lambda_1\mathbf{v_1}te^{\lambda_1 t} + \lambda_1\mathbf{v_2}e^{\lambda_1 t} = \mathbf{P}\mathbf{v_1}te^{\lambda_1 t} + \mathbf{P}\mathbf{v_2}e^{\lambda_1 t}$$

Using the fact that $\mathbf{P}\mathbf{v_1} = \lambda_1\mathbf{v_1}$:

$$\mathbf{v_1}e^{\lambda_1 t} + \lambda_1\mathbf{v_1}te^{\lambda_1 t} + \lambda_1\mathbf{v_2}e^{\lambda_1 t} = \lambda_1\mathbf{v_1}te^{\lambda_1 t} + \mathbf{P}\mathbf{v_2}e^{\lambda_1 t}$$

We can subtract both sides by  $\lambda_1\mathbf{v_1}te^{\lambda_1 t}$, to get:

$$\mathbf{v_1}e^{\lambda_1 t} + \lambda_1\mathbf{v_2}e^{\lambda_1 t} = \mathbf{P}\mathbf{v_2}e^{\lambda_1 t}$$

Moving terms and switching sides:
$$\mathbf{P}\mathbf{v_2}e^{\lambda_1 t} - \lambda_1\mathbf{v_2}e^{\lambda_1 t} = \mathbf{v_1}e^{\lambda_1 t}  $$

Since $e^{\lambda_1 t}$ is always greater than 0, we can divide both sides by it:
$$\mathbf{P}\mathbf{v_2} - \lambda_1\mathbf{v_2} = \mathbf{v_1} $$

We can factor out $\mathbf{v_2}$:

$$(\mathbf{P}-\lambda_1\mathbf{I})\mathbf{v_2} = \mathbf{v_1} $$

Also, since $\mathbf{v_1}$ satisfies:

$$(\mathbf{P}-\lambda_1\mathbf{I})\mathbf{v_1} = 0$$

Plugging in the expression for $v_1$ in the above expression:

$$(\mathbf{P}-\lambda_1\mathbf{I})(\mathbf{P}-\lambda_1\mathbf{I})\mathbf{v_2} = 0$$
$$(\mathbf{P}-\lambda_1\mathbf{I})^2\mathbf{v_2} = 0$$


Thus, if we can solve for $\mathbf{v_2}$ using the expression above, we can in fact solve for another linearly independent solution! The vector $\mathbf{v_2}$ is in fact called a \textbf{generalized eigenvector} to the matrix $P$.


It turns out that we can continue this process if the multiplicity of the eigenvalues is greater than 2:

\begin{mytheo}{Repeated Eigenvalues}{repEig}
    Given the system of linear differentiable equations:
    
    $$\mathbf{x'} = \mathbf{Px}$$
    
    If we solve for the eigenvalues of $P$ and get $k$ repeats (multiplicity of $k$), but $< k$ linearly independent eigenvectors, we can find linearly independent solutions by computing the generalized eigenvectors defined by:
    $$(\mathbf{P}-\lambda\mathbf{I})^j\mathbf{v_j} = \mathbf{0} $$
    $$(\mathbf{P}-\lambda\mathbf{I})^{j-1}\mathbf{v_j} \neq \mathbf{0}$$
    for $j = 1, \dots, k$
    such that:
    $$(\mathbf{P}-\lambda\mathbf{I})\mathbf{v_1} = \mathbf{0}$$
    $$(\mathbf{P}-\lambda\mathbf{I})\mathbf{v_2} = \mathbf{v_1}$$
    $$\vdots$$
    $$(\mathbf{P}-\lambda\mathbf{I})\mathbf{v_k} = \mathbf{v_{k-1}}$$
    
    Once we have the generalized eigenvectors, we can find $k$ linearly independent solutions by:
    $$\mathbf{x_1}(t) = \mathbf{v_1}e^{\lambda t}$$
    $$\mathbf{x_2}(t) = (\mathbf{v_2} + t\mathbf{v_1})e^{\lambda t}$$
    $$\vdots$$
    $$\mathbf{x_k}(t) = \left(\mathbf{v_k} + t\mathbf{v_{k-1}} + \frac{t^2}{2}\mathbf{v_{k-2}} + \dots + \frac{t^{(k-1)}}{(k-1)!}\mathbf{v_1}\right)e^{\lambda t}$$
\end{mytheo}



\section{Visualization of 2x2 solutions and Trace-Determinant Plane}

Let's examine how particular solutions to the eigenvalues of a 2x2 coefficient matrix look geometrically:


\subsection{Nodal sources and sinks and saddle points}

In the case of real eigenvalues, our solution will be in the form:

$$\mathbf{x} = \mathbf{v_1}e^{\lambda_1 t} + \mathbf{v_2}e^{\lambda_2 t}$$

What happens geometrically when both $\lambda_1, \lambda_2 > 0$?

If we plot the solution we see that as time goes on, we are repelled from the center at any starting point:

\includegraphics[scale=0.5]{nodal_source.png}

This is called a \textbf{nodal source}

Similarly if $\lambda_1,\lambda_2 < 0$, we get:

\includegraphics[scale=0.5]{nodal_sink.png}

This is called a \textbf{nodal sink} since as time goes by we go toward the eigenvectors.



If we have opposite signs, we get something called a \textbf{saddle point}:

\includegraphics[scale=0.5]{saddle.png}


\subsection{Centers and Spirals}
\label{sec:center}
In this section we will examine the case where we get complex eigenvalues.

\hyperref[sec:prCenter]{The proof of the following fact is given in the appendix.}


In the case of a purely imaginary eigenvalue $\lambda = i\mu$, the complete solution can be described by:

$$
\mathbf{x(t)} = 
\begin{bmatrix}
    A_1\cos(\mu t - \phi_1) \\ 
    A_2\sin(\mu t - \phi_2) \\
\end{bmatrix}
$$

This is just an ellipse! with a center determined by $\phi_1, \phi_2$ and sides $A_1, A_2$. This is called a \textbf{center}:


\includegraphics[scale=0.4]{center.png}


What if there is in fact a non-zero real component to the complex eigenvalue?

$$\lambda = r + i\mu$$

Then we have:
$$\mathbf{x(t)} = c_1\Re{\mathbf{v_1}e^{(r+i\mu) t}} + c_2\Im{\mathbf{v_1}e^{(r+i\mu) t}}$$

which is just:


$$\mathbf{x(t)} = e^{rt}[c_1\Re{\mathbf{v_1}e^{i\mu) t}} + c_2\Im{\mathbf{v_1}e^{i\mu t}}]$$


The right side of this expression is identical to before. However, now we have an exponential term to deal with:


$$
\mathbf{x(t)} = 
e^{rt}
\begin{bmatrix}
    A_1\cos(\mu t - \phi_1) \\ 
    A_2\sin(\mu t - \phi_2) \\
\end{bmatrix}
$$

You could think about this as an exponentially increasing or decreasing amplitude for the ellipse. Thus this forms a spiral!

If the eigenvalues are:
$\lambda = r \pm i\mu$ and $r>0$, then we get a \textbf{spiral source}:

\includegraphics[scale=0.3]{spiral_source.png}


Otherwise if the eigenvalues are:
$\lambda = r \pm i\mu$ and $r<0$, then we get a \textbf{spiral sink}:

\includegraphics[scale=0.3]{spiral_sink.png}

\subsection{Combs, Degenerate Node and Star Nodes}

What happens when we have one of the eigenvalues to be 0?

We get something like this called a \textbf{comb}. It can be a \textbf{repelling comb} or an \textbf{attracting comb} depending on whether the non-zero eigenvalue is positive or negative respectively (the figure is of attracting comb):.

\includegraphics[scale=0.4]{comb.png}

What happens when we have repeated roots?

\hyperref[sec:repEigen]{In our discussion above} we described two cases. One where even though we had $k$ repeated eigenvalues, we were still able to find $k$ linearly independent eigenvectors. It turns out that this is the case when $\mathbf{P} = \lambda I$. Thus the solution will simply be:

$$\mathbf{x}(t) = 
c_1
\begin{bmatrix}
    1 \\
    0
\end{bmatrix}
e^{\lambda t}
+
c_2
\begin{bmatrix}
    1 \\
    0
\end{bmatrix}
e^{\lambda t}
$$

which forms a \textbf{star node} when we plot it:

\includegraphics[scale=0.4]{star_node.png}

Notice that this can be attracting or repelling depending on whether or not $\lambda < 0$ or $\lambda > 0$ respectively.

There are some cases, called degenerate cases, where we get repeated eigenvalues without linearly independent eigenvectors. In this case two find the two linearly independent solutions to span the solution space, we need to find the \hyperref[th:repEig]{generalized eigenvectors as described above}.

Thus, the complete solution in this case is given by:

$$\mathbf{x}(t) = c_1\mathbf{v_1}e^{\lambda t} + c_2(t\mathbf{v_1} + \mathbf{v_2})e^{\lambda t}$$

When we plot this out, we see something that looks like it is in between a spiral and a node called a \textbf{degenerate node}:

\includegraphics[scale=0.5]{degenerate.png}

This is also attracting or repelling depending on the sign of $\lambda$.


Let's put our results together:


\begin{tabular}{ c c c}
 \textbf{Name} & $\mathbf{\lambda_1, \lambda_2}$  & \textbf{Description} \\
 \hline
 Nodal source/sink & \lambda_1\lambda_2 > 0 & $\lambda_1, \lambda_2$ have same sign \\
 Saddle node & \lambda_1\lambda_2 < 0 & $\lambda_1, \lambda_2$ have different signs\\
 Center & $\lambda_1,\lambda_2 = \pm \mu i$ & $\lambda_1,\lambda_2$ are  purely imaginary \\
 Spiral & $\lambda_1,\lambda_2 =  r \pm \mu i$ & $\lambda_1,\lambda_2$ are complex with a real part \\
 Comb & $\lambda_1 = 0,\lambda_2 \neq 0$ & One $\lambda_1$ is zero while the other is non-zero \\
 Star node & $\lambda_1=\lambda_2$ & Repeated eigenvalue with $\mathbf{P} = \lambda_1\mathbf{I}$
 \\
 Degenerate node & $\lambda_1=\lambda_2$ & Repeated eigenvalue with $\mathbf{P} \neq \lambda_1\mathbf{I}$
\end{tabular}


\subsection{Trace-Determinant Plane}
We know that our 2x2 system looks like
$$
\begin{bmatrix}
    x' \\
    y' \\
\end{bmatrix}
=
\begin{bmatrix}
    p_{11} & p_{12} \\
    p_{21} & p_{22} \\
\end{bmatrix}
\begin{bmatrix}
    x \\
    y \\
\end{bmatrix}
$$

We know that from our previous discussion that the eigenvalues of the matrix $\mathbf{P}$ determine the solution of this system.

The eigenvalues of a matrix can be solved by solving the following equality:

$$\det(\mathbf{P}-\lambda \mathbf{I}) = \mathbf{0}$$

In our case, this is just:

$$
\text{det}\left(
\begin{bmatrix}
    p_{11} - \lambda & p_{12} \\
    p_{21} & p_{22} - \lambda \\
\end{bmatrix}\right) = 0
$$

Using the formula for calculating determinants, we get:

$$( p_{11} - \lambda)(p_{22} - \lambda)- p_{12}p_{21} = 0$$

This simplifies to:

$$\lambda^2 - (p_{11} + p_{22})\lambda + (p_{11}p_{21}- p_{12}p_{21}) = 0$$

Notice that $p_{11} + p_{22}$ is just the trace of the original coefficient matrix $\mathbf{P}$. Also, $p_{11}p_{21}- p_{12}p_{21}$ is just the determinant of the coefficient matrix.

Thus, the eigenvalues of a 2x2 system is just the solutions to the quadratic:

$$\lambda^2-\Tr(\mathbf{P})\lambda + \det(\mathf{\mathbf{P}}) = 0$$

Using the quadratic formula:

$$\lambda = \frac{\Tr(\mathbf{P}) \pm \sqrt{\Tr(\mathbf{P})^2-4\det(\mathbf{P}) }}{2}$$

This means that whether or not the eigenvalues are real, complex, or repeated is determined entirely by the term $\Tr(\mathbf{P})^2-4\det(\mathbf{P})$!

The eigenvalues are real and unique when:

$$\det(\mathbf{P}) < \frac{\Tr(\mathbf{P})^2}{4}$$

They are complex and unique when:

$$\det(\mathbf{P}) < \frac{\Tr(\mathbf{P})^2}{4}$$

They are repeated reals when:

$$\det(\mathbf{P}) = \frac{\Tr(\mathbf{P})^2}{4}$$

We can thereby graph our results on a trace-determinant plane.

\includegraphics[scale=0.3]{tr-det.png}

If, after we find the trace and determinant of a matrix, it is below the parabola outlined, then we have two unique real roots. If it is on the parabola, we have a repeated root. If it is above the parabola, we have a complex conjugate pair of eigenvalues.


Furthermore, the discriminant of the quadratic expression also tells us the sign of the eigenvalues.

Thus, it is possible to view all the node types on the determinant-trace plane as follows:

\includegraphics[scale=0.5]{trace_det_full.png}



























\section{Fundamental Matrix and Matrix Exponential}
\section{Non-homogeneous Case}
\section{Approximating Non-linear Differential Equations}


\chapter{Series Solutions}

\chapter{Laplace Transform}

\chapter{First-order case}

\chapter{Questions}
\begin{itemize}
    \item Why does an nth order linear equation with n initial conditions need n linearly independent solutions to fully describe the solution space?
    \item Proof of Uniqueness and Existence theorem (or at least intuition)?
    \item What is Bessels equation?
    \item Is there a form of Abel's equation in higher than 2 dimensions?
    \item Why does putting $t, t^2, \dots $ before solution to a constant coefficient differential homogeneous give us another linearly independent solution for any degree?
    \item Why is a generalized eigenvector guaranteed to exist if we have a eigenvalue with multiplicity greater than one.
\end{itemize}

\chapter{Proofs}
\section{\hyperref[th:wronLinInd]{Proof of Wronskian implying linear independence}}
\label{sec:prWronLinInd}
Consider the equation 
$$c_1\bm{f(t)} + c_2\bm{g(t)} = 0$$ 
for some interval $t \in I$. 

We know that is we can find non-zero $c_1$ and $c_2$ such that this equation is always true, the functions $\bm{f(t)}$ and $\bm{g(t)}$ are linearly dependent. On the other hand, if we can find some $t_0 \in I$ such that the only way we can satisfy this equation is by setting $c_1 = c_2 = 0$, then the functions are linearly independent.

Say that the functions have their derivatives defined. Thus, we have a set of equations:
$$c_1\bm{f(t)} + c_2\bm{g(t)} = 0$$ 
$$c_1\bm{f'(t)} + c_2\bm{g'(t)} = 0$$ 

In matrix form:
$$
\begin{bmatrix}
    \bm{f(t)} & \bm{g(t)} \\
    \bm{f'(t)} & \bm{g'(t)} \\
\end{bmatrix}
\begin{bmatrix}
    c_1 \\
    c_2 \\
\end{bmatrix}
=
\begin{bmatrix}
    0 \\
    0 \\
\end{bmatrix}
$$

If the determinant of the matrix (which is the Wronskian) is non-zero for some $t_0 \in I$, we know from basic linear algebra, that the only $\begin{bmatrix}
    c_1 \\
    c_2 \\
\end{bmatrix}$ that satisfies the equation is $\begin{bmatrix}
    0 \\
    0 \\
\end{bmatrix}$ (there is no non-zero null space for the matrix).

Thus, when the \textbf{Wronskian} is non-zero for some $t_0$, the functions must be linearly independent since it is impossible to find a non-zero $c_1$ and $c_2$ that satisfy:
$$c_1\bm{f(t_0)} + c_2\bm{g(t_0)} = 0$$
$$c_1\bm{f'(t_0)} + c_2\bm{g'(t_0)} = 0$$.






\section{\hyperref[th:abel_thm]{Proof of Abel's Theorem}}
\label{sec:abel}
Say we have the differential equation.
$$y''(t) + p_1(t)y'(t) + p_0(t)y(t) = 0$$
Say we also have two solutions $y_1(t)$ and $y_2(t)$ that satisfy this equation:
$$y_1''(t) + p_1(t)y_1'(t) + p_0(t)y_1(t) = 0$$
$$y_2''(t) + p_1(t)y_2'(t) + p_0(t)y_2(t) = 0$$

Let's multiply the top equation by $y_2(t)$ and the bottom equation by $y_1(t)$ and subtract to get rid of the term with $p_0(t)$:

$$y_1''(t)y_2(t) + p_1(t)y_1'(t)y_2(t) + p_0(t)y_1(t)y_2(t) = 0$$
$$y_2''(t)y_1(t) + p_1(t)y_2'(t)y_1(t) + p_0(t)y_2(t)y_1(t) = 0$$

Subtracting yields:
$$y_1''(t)y_2(t) + p_1(t)y_1'(t)y_2(t) - (y_2''(t)y_1(t) + p_1(t)y_2'(t)y_1(t)) = 0$$
which after rearranging and simplifying yields:
\begin{equation}
\label{abel_eq}
(y_1''(t)y_2(t)-y_2''(t)y_1(t))+p_1(t)(y_1'(t)y_2(t)-y_2'(t)y_1(t)) = 0
\end{equation}

multiplying by $-1$ and simplifying some more:
$$(y_1(t)y_2''(t)-y_2(t)y_1''(t))+p_1(t)(y_1(t)y_2'(t)-y_2(t)y_1'(t)) = 0$$
The term $y_1(t)y_2'(t)-y_2(t)y_1'(t)$ looks familiar. In fact, it is the Wronskian of $y_1(t)$ and $y_2(t)$:
$$
    W(y_1(t), y_2(t))(t) =
    \text{det}\left(\begin{bmatrix}
        y_1(t) & y_2(t) \\
        y_1'(t) & y_2'(t) \\
        \end{bmatrix}\right)
    = y_1(t)y_2'(t)-y_2(t)y_1'(t)
$$
It also turns out that (by the chain rule):
$$
W'(y_1(t), y_2(t))(t) = y_1'(t)y_2'(t)-y_2(t)y_1''(t)+y_1(t)y_2''(t)-y_2'(t)y_1'(t)
$$
$$=y_1(t)y_2''(t)-y_2(t)y_1''(t)$$

Returning to equation \ref{abel_eq}, we can substitute in the Wronskian and the derivative to get:
$$W'(y_1(t),y_2(t))(t) + p_1(t)W(y_1(t), y_2(t))(t) = 0$$

This is a separable first order equation for $W(t)$. Thus,

$$\frac{W'(y_1(t),y_2(t))(t)}{W(y_1(t), y_2(t))(t)} = -p_1(t)$$
Integrating both sides and simplifying:
$$\int{\frac{dW}{W(y_1(t), y_2(t))(t)} }= \int{-p_1(t)dt}$$
$$=\ln|W(y_1(t), y_2(t))(t)| = \int{-p_1(t)dt} + c$$
$$W(y_1(t), y_2(t))(t)| = \exp{\left[\int{-p_1(t)dt} + c\right]}$$
$$=c\exp{\left[-\int{p_1(t)dt}\right]}$$


\section{\hyperref[th:WrLinDep]{Proof of Wronskian and Linear Dependence Relationship}}
\label{sec:PrWrLinDep}
This theorem says two main things:
\begin{enumerate}
    
    \item$W(y_1(t), y_2(t))(t) = 0 \implies \text{Linear Dependence}$
    
    \item$ \text{Linear Dependence} \implies W(y_1(t), y_2(t))(t) = 0$
\end{enumerate}
We know that the second statement is true from \hyperref[th:wronLinDep]{this theorem.} The challenge in this proof is to prove the first statement.

Let our two solutions be $y_1(t)$ and $y_2(t)$. Say we have the equation:
$$c_1y_1(t) +c_2y_2(t) = 0$$
The constants $c_1$ and $c_2$ can be zero or non-zero to satisfy this equation. If we can find a $c_1$ and $c_2$ such that they are both non-zero and it always satisfies this solution for any $t$ in some interval $I$, then $y_1(t)$ and $y_2(t)$ are linearly dependent.

From our equation we can take derivatives of both sides to get:
$$c_1y_1(t) +c_2y_2(t) = 0$$
$$c_1y_1'(t) +c_2y_2'(t) = 0$$

Which forms the linear equations:
$$
\begin{bmatrix}
    y_1(t) & y_2(t) \\
    y_1'(t) & y_2'(t) \\
\end{bmatrix}
\begin{bmatrix}
    c_1 \\
    c_2 \\
\end{bmatrix}
=
\begin{bmatrix}
    0 \\
    0 \\
\end{bmatrix}
$$

This equation will be satisfied for some $c_1$ and $c_2$ if and only if the determinant of the matrix is 0. That is when:
$$
\text{det}\left(\begin{bmatrix}
        y_1(t) & y_2(t) \\
        y_1'(t) & y_2'(t) \\
        \end{bmatrix}\right)
$$
which is just the Wronskian.

We might think that completes our proof, however, this is not the case. We've proven that we can always find a non-zero $c_1$ and $c_2$, however, they may not be the \textbf{same} $c_1$ and $c_2$ throughout the equation. 

For example, in the case of the functions $t^3$ and $|t|t^2$ we can in fact find a $c_1$ and $c_2$ such that we always satisfy $c_1t^3+c_2|t|t^2=0$ for any t. However, for $t < 0$, $c_1=c_2=1$ satisfies this equation. In the case of $t>0$, $c_1=-c_1$ will satisfy this equation. Therefore these equations are not linearly dependant even though their Wronskian will be 0 everywhere.

So in order to complete our proof, we need to make use of an additional fact, that $y_1(t)$ and $y_2(t)$ are solutions of a differential equation. 

Say that for the differential equation $y_1(t)$ and $y_2(t)$ are the solution to we have the following arbitrarily initial values on the complete solution $\phi(t)$
$$\phi(t_0) = 0$$
$$\phi'(t_0) = 0$$


We also know that $y_1(t)$ and $y_2(t)$ satisfy this differential equation (by definition) and by superposition, any linear combination of them must also satisfy this equation. Thus, let us consider a possible  general solution to be:

$$\phi(t) = c_1y_1(t) +c_2y_2(t)$$

Using the initial constraints,

$$\phi(t_0) = c_1y_1(t_0) +c_2y_2(t_0) = 0$$
$$\phi'(t_0) = c_1y_1'(t_0) +c_2y_2'(t_0) = 0$$

which solves the matrix equation that we had earlier:
$$
\begin{bmatrix}
    y_1(t_0) & y_2(t_0) \\
    y_1'(t_0) & y_2'(t_0) \\
\end{bmatrix}
\begin{bmatrix}
    c_1 \\
    c_2 \\
\end{bmatrix}
=
\begin{bmatrix}
    0 \\
    0 \\
\end{bmatrix}
$$

If the Wronskian is 0, we can always find $c_1$ and $c_2$ that are both non-zero satisfying this equation (a fact we established earlier). 

Therefore, $\phi(t) = c_1y_1(t) + c_2y_2(t)$ is a solution the differential equation with the initial constraints $\phi(t_0) = 0$ and $\phi'(t_0) = 0$. if the Wronskian of $y_1(t)$ and $y_2(t)$ is zero. 

Another interesting fact is that the trivial solution $\phi(t) = 0$ also satisfies our initial constraints $\phi(t_0) = 0$ and $\phi'(t_0) = 0$.

By the  \hyperref[th:Ex&Un]{Existence and Uniqueness theorem}, if we find one solution satisfying the initial value problem, it must be the only solution the differential equation. Since we found two solutions to the differential equation we're considering, they must be equal to each other for all $t \in I$.
Thus,
$$\phi(t) = c_1y_1(t) +c_2y_2(t) = 0$$
for all $t \in I$

Taking derivatives of both sides and putting into matrix form, we see that:

$$
\begin{bmatrix}
    y_1(t) & y_2(t) \\
    y_1'(t) & y_2'(t) \\
\end{bmatrix}
\begin{bmatrix}
    c_1 \\
    c_2 \\
\end{bmatrix}
=
\begin{bmatrix}
    0 \\
    0 \\
\end{bmatrix}
$$

for $c_1, c_2 \neq 0$, $t \in I$

Thus, when the Wronskian is 0 for two solutions of a second order linear homogeneous differential equation, we can find a universal $c_1$ and $c_2$ that are both non-zero such that $c_1y_1(t) +c_2y_2(t) = 0$ is always satisfied. 


\section{\hyperref[th:expLinInd]{Proof of Linear Independence of exponentials of different degrees}}
\label{sec:prExpLinInd}
We know that  if we show that the Wronskian of two functions $y_1(t)$ and $y_2(t)$ is non-zero at any point, \hyperref[th:wronLinInd]{the functions must be linearly independent.}

Thus, if we show that the Wronskian of $e^{r_1t}$ and $e^{r_2t}$ is 0 for $r_1 \neq r_2$ then they must be linearly independent.

Let's do a proof by contradiction. Suppose that the $r_1 \neq r_2$, but the Wronskian is 0.Thus,

$$
    \text{det}\left(\begin{bmatrix}
        e^{r_1t} & e^{r_2t} \\
        r_1 e^{r_1t} & r_2 e^{r_2t} \\
    \end{bmatrix}\right)
    = 0
$$
Thus,by the definition of a determinant,
$$r_2e^{r_2t}e^{r_1t} -r_1e^{r_1t}e^{r_2t} = 0$$
which simplifies to:
$$(r_2-r_1)e^{(r_1+r_2)t} = 0$$
Since $e^{(r_1+r_2)t} \neq 0$, we can divide both sides by it and get:
$$r_2 -r_1 = 0$$
$$r_2 = r_1$$

which contradicts our assumption that $r_1 \neq r_2$

Thus, the Wronskian is in fact non-zero and the functions $e^{r_1t}$ and $e^{r_2t}$ are linearly independent.

\section{\hyperref[sinLin]{Proof that sine and cosine are linearly independent}}
\label{sec:prSinLin}
If we can show that Wronskian of sine and cosine is non-zero at any particular $t_0$, then the functions are linearly independent.

Let's take the Wronskian of $y_1(t) = \sin(bt)$ and $y_2(t) = \cos(bt)$

$$
    \text{det}\left(\begin{bmatrix}
        \sin(bt) & b\cos(bt)\\
        b\cos{bt} & -b\sin(bt) \\
    \end{bmatrix}\right)
$$
$$=-b\sin^2(bt)-b\sin^2(bt)$$
$$=-b(\sin^2(bt) + \cos^2(bt)$$
By the elementary trig identity $\sin^2(bt) + \cos^2(bt) = 1$, we get:
$$W(\sin(bt),\cos(bt))(t)=-b$$
which is non-zero everywhere when $b$ is non-zero.

Because, the Wronskian indeed is 0 everywhere, the functions $\sin(bt)$ and $\cos(bt)$ are linearly independent


\section{\hyperref[repRoot]{Proof of finding Linearly Independent solutions for repeated roots (2nd Order)}}
\label{sec:PrRepRoot}
Say that we have a homogeneous linear differential equation of 2nd order with constant coefficients:

$$y''(t) + a_1y'(t) + a_0y = 0$$

Say that we solve for the characteristic polynomial and get an expression in the form of:

$$r^2 + a_1r + a_0 = 0$$

Furthermore, say that when we solve this polynomial (for example, using the quadratic equation), we get two repeated roots $r_1=r_2$.

We need to find another linearly independent solution if we wish to span the complete solution space.

We know that $y(t) = Ce^{r_1t}$ must be a solution (since it always satisfies the equation. Let's see if we can multiply it by a function $u(t)$ and see if the complete function satisfies our original differential equation.

Thus, another potential solution is:

$$y_2(t) = u(t)e^{rt}$$

Plugging that in gives us:
$$(u(t)e^{rt})'' + a_1(u(t)e^{rt})' + a_0(u(t)e^{rt}) = 0$$

Applying the chain rule gives us:
$$(u''(t)e^{rt}+2u'(t)(re^{rt}) + u(t)(r^2e^{rt})) + a_1(u'(t)e^{rt} + u(t)(re^{rt}))' + a_0(u(t)e^{rt}) = 0$$
Collecting terms:
$$u(t)e^{rt}(r^2+a_1r+a_0) +u''(t)e^{rt} + 2u'(t)(re^{rt}) + a_1 u(t)(re^{rt}) = 0$$
Since we know $r$ satisfies the characteristic polynomial equation, $(r^2+a_1r+a_0)$ must be 0. Thus,
$$u''(t)e^{rt} + 2u'(t)(re^{rt}) + a_1 u(t)(re^{rt}) = 0$$
Simplifying by dividing by $e^{rt}$ (which is never 0) yields,
$$u''(t) + u'(t)(2r+a_1) = 0$$

Hmm, what should we do now?

We know that the roots of a quadratic in the form $x^2 + bx + c = 0$ are given by the quadratic formula:
$$r = \frac{-b \pm \sqrt{b^2-4c}}{2}$$
When we have repeated roots, the discriminant $b^2-4ac$ is 0, giving us:
$$r = \frac{-b}{2}$$


Putting this in our equation yields:
$$u''(t)+u'(t)(2r+a_1) = 0$$
$$u''(t)+u'(t)(2(\frac{-a_1}{2}+a_1) = 0$$
$$u''(t) + u'(t)(-a_1+a_1) = 0$$
$$u''(t) = 0$$

Integrating we get:
$$u'(t) = c$$
$$u(t) = ct + d$$

Plugging back into original equation, we get:
$$y_2(t) = (ct+d)e^{rt}$$

We can remove the $de^{rt}$ part, since that is already a linear multiple of our first solution.

Is the equation, $y_2(t) = te^{rt}$ linearly independent with $y_1(t) = e^{rt}$

We can take the Wronskian and see if it is non-zero at any point in the interval we're considering:


$$
    \text{det}\left(\begin{bmatrix}
        e^{rt} & te^{rt}\\
        re^{rt} & rte^{rt}+ te^{rt} \\
    \end{bmatrix}\right)
$$
$$
    = e^{rt}(rte^{rt}+te^{rt}) - te^{rt}re^{rt}
    = (e^{rt})^2(rt+t-rt)
    = te^{rt}
$$
which is in fact non-zero everywhere where $t$ is non-zero!

\section{\hyperref[th:sinPlusCosin]{Proof of Ssum of Sine and Cosine of same frequency}}
\label{sec:prSinPlusCosin}
First proof:

Consider taking the dot product between two vectors $(a,b)$ and a vector arbritrarily defined on the unit circle $(\cos(\theta),\sin(\theta))$

$$(a,b)\cdot(\cos(\theta),\sin(\theta)) = a\cos(\theta) + b\sin(\theta)$$

We know that by the geometric interpretation of the dot product, that:

$$(a,b)\cdot(c,d) = \lVert(a,b)\rVert*\lVert(c,d)\rVert*\cos(\text{angle b/w vectors})$$

Thus, the dot product of $(a,b)$ and $(\cos(\omega t),\sin(\omega t))$ is just:

$$(a,b)\cdot(\cos(\theta),\sin(\theta)) = \lVert(a,b)\rVert*\lVert(\cos(\theta),\sin(\theta)\rVert*\cos(\text{angle b/w vectors})$$

We know that $\lvert(\cos(\theta),\sin(\theta))\rvert =1$ so this simplifies to 

$$\lVert(a,b)\rVert*\cos(\text{angle b/w vectors})$$

The magnitude of $(a,b)$ is simply $\sqrt{a^2+b^2}$

The angle between both vectors is just going to be the difference between the angle created by the vector $(a,b)$ and $\theta$. 

Using basic trigonometry, the angle of the vector $(a,b)$ is $\phi = \tan^{-1}(b/a)$

Thus we get:

$$(a,b)\cdot(\cos(\theta),\sin(\theta) = A\cos(\theta - \phi)$$

where
$$A = \sqrt{a^2+b^2}$$
$$\phi = \tan^{-1}(b/a)$$

Since both interpretations of the dot product yield the same result, we get:

$$a\cos(\theta) + b\sin(\theta) = A\cos(\theta - \phi)$$

[INSERT PICTURE]

Second proof:

It turns out that

$$a\cos(\theta) + b\sin(\theta) = \Re{(a-bi)*(\cos(\theta) + i\sin(\theta)}$$

(you can quickly verify this!)

We know that the expression $a-bi$ like all complex numbers, can be represented in the form $Ae^{i\phi}$

In this case $A$ is the magnitude of the complex number, which is just $\sqrt{a^2+b^2}$.

In addition, $\phi$ is the angle formed by the real and imaginary part of the complex number. This will be negative since there is a negative imaginary part in the expression we're considering. In this case the angle between these parts is $\phi = \tan^{-1}(b/a)$, but we will be considering $-\phi$ since we want the negative.

We also know that from Euler's identity that:

$$\cos(\theta) + i\sin(\theta) = e^{i\theta}$$

Plugging these substitutions, we get:

$$a\cos(\theta) + b\sin(\theta) = \Re{Ae^{-i\phi}e^{i\theta}}$$
$$=A\Re{e^{(\theta-\phi)t}}$$
which by Euler's Identity is:
$$=A\Re{\cos((\theta-\phi)t)+i\sin((\theta-\phi)t)}$$
$$=A\cos((\theta-\phi)t)$$

where 
$$\phi = \tan^{-1}(b/a)$$
$$A = \sqrt{a^2+b^2}$$




\section{\hyperref[th:nhSol]{Proof of Non-homogeneous Solution}}
\label{sec:prNhSol}
Say we have the differential equation:
 $$L[y] = y^{(n)}(t) + ... + p_1(t)y'(t) + p_0(t)y(t) = g(t)$$
and that we find two independent solutions that satisfy this equation, let's call them $y_1(t) and y_2(t)$.
Thus, $L[y_1]=g(t)$ and $L[y_2]=g(t)$
What happens when we subtract $L[y_1] and L[y_2]$?
we get $L[y_1]-L[y_2] = 0$
We know by linearity that 
$$L[y_1]-L[y_2] = L[y_1-y_2] = 0$$
this means that the combination $y_1(t)-y_2(t)$ solves the homogeneous equation:
$$L[y] = y^{(n)}(t) + ... + p_1(t)y'(t) + p_0(t)y(t) = 0$$
Thus,
$$y_h(t) = y_1(t)-y_2(t)$$
$$y_1(t) = y_h(t) + y_2(t)$$

We find that our two "independent" solutions are actually the same plus a homogeneous solution.

Since all the potential solutions can in fact be decomposed this way, our complete solution is in fact the sum of 1 particular solution and all the homogeneous solutions of the differential equation.


\section{\hyperref[th:polyResp]{NH Form of Solution with Polynomial Response}}
\label{sec:prPolyResp}
Consider a differential equation in the form:

$$y^{(n)}(t) + \dots + c_1y'(t) + c_0y(t) = a_0t^n + a_1t^{n-1} + \dots a_n$$

If we put in a particular solution of the form:

$$y_p(t) = A_0t^n + \dots + A_{n-1}t + A_n$$

into our differential equation, will get a system of equations in which we can equate all the $t, t^2, \dots t^n$ in the LHS with the RHS.

Thus substituting we get something like:

$$[n!A_0] + \dots c_1[A_0nt^{n-1}+\dots+A_{n-1}] + c_0[A_0t^n + \dots + A_{n-1}t + A_n]$$

$$= a_0t^n + a_1t^{n-1} + \dots + a_n$$

We can equate coefficients of the same power of $t$ to get the following system of equations:

$$c_0A_0t^n = a_0t^n$$
$$(c_1A_0n + c_0A_1)t^{n-1} = a_1t^{n-1}$$
$$\vdots$$
$$n!A_0 + \dots + c_0A_n = a_n$$


If we solve for $A_0$ using the first equation, it is possible to solve for $A_1$ in the second equation and so on until $A_n$

However, if we cannot solve for $A_0$, then we cannot find any of the other coefficients and must make some sort of adjustment.

What determines whether or not we can find $A_0$? By looking at the first equation, we can see that $A_0$ can be solved for when $c_0 \neq 0$ ($A_0 = \frac{a_0}{c_0}$).

What should we do when $c_0 = 0$?

If try the solution
$$y_p(t) = t(A_0t^n + \dots + A_{n-1}t + A_n)$$
$$=A_0t^{n+1} + \dots + A_{n-1}t^2 + A_nt$$


Plugging this into our original differential equation and equating coefficients we get:

$$[A_0(n+1)!t+A_1(n+1)!] + \dots + c_1[A_0(n+1)t^n+\dots + A_n]$$
$$= a_0t^n + a_1t^{n-1} + \dots + a_n$$

Equating coefficients like before:

$$c_1A_0(n+1)t^n = a_0t^n$$
$$(c_1A_1n+c_2A_0)t^{n-1} = a_1t^{n-1}$$
$$\vdots$$
$$(n+1)!A_0 + \dots + c_1A_{n} = a_n$$

In this case, if $c_1 \neq 0$, we can solve for $A_0$ and all the rest of the coefficients $A_1, \dots, A_n$ quite easily!

When $c_0=c_1=0$, we can see that in fact, this method does not give us a solution either :(. However, we can repeat what we did earlier by multiplying another $t$ and plug:

$$y_p(t) = t^2(A_0t^n + \dots + A_{n-1}t + A_n)$$
$$=A_0t^{n+2} + \dots + A_{n-1}t^3 + A_nt^2$$

as a potential solution.

We can continue this process arbitrarily large $s$ such that $c_0=c_1=\dots=c_s=0$ and see that this process continues to yield us solutions for $A_0, \dots, A_n$


\section{\hyperref[th:var]{Proof of Variation of Parameters}}
\label{sec:prVar}
We have a differential equation of the form:

$$y^{(n)}(t) + \dots + p_1(t)y'(t) + p_0(t)y(t) = g(t)$$

Say that the n solutions to the homogeneous are:
$y_1(t), y_2(t), \dots, y_n(t)$ respectively.

The method of variation of parameters relies on using these non-homogeneous solutions as "jumping points" to find a non-homogeneous solution of the form:

$$y_p(t) = u_1(t)y_1(t) + u_2(t)y_2(t) + \dots + u_n(t)y_n(t)$$

where $u_1(t), \dots, u_n(t)$ are arbitrary functions of just that we set to satisfy the non-homogeneous equation.

What is the derivative of this function. By the product rule with some rearranging, we get:

$$y_p'(t) = (u_1(t)y_1'(t) + u_2(t)y_2'(t) + \dots
+ u_n(t)y_n'(t)) $$  $$+ (u_1'(t)y_1(t) + u_2'(t)y_2(t) + \dots + u_n'(t)y_n(t))$$

Let's arbitrarily set the second part of this equation to 0:

$$u_1'(t)y_1(t) + u_2'(t)y_2(t) + \dots + u_n'(t)y_n(t) = 0$$

We can do this because we are in fact assigning what $u_1(t), \dots, u_n(t)$ should in fact be. We're doing this because it makes taking subsequent derivatives easier, as we will see.

Thus,

$$y_p'(t) = u_1(t)y_1'(t) + u_2(t)y_2'(t) + \dots + u_n(t)y_n'(t)$$

What is the second derivative?

$$y_p''(t) = (u_1(t)y_1''(t) + u_2(t)y_2''(t) + \dots + u_n(t)y_n''(t))$$ 
$$ +  (u_1'(t)y_1'(t) + u_2'(t)y_2'(t) + \dots + u_n'(t)y_n'(t))$$

We can again arbitrarily set the second expression to 0:
$u_1'(t)y_1'(t) + u_2'(t)y_2'(t) + \dots + u_n'(t)y_n'(t)) = 0$

We can continue this process such that the first $n$ derivatives are:

$$y_p(t) = u_1(t)y_1(t) + u_2(t)y_2(t) + \dots + u_n(t)y_n(t)$$
$$y_p'(t) = u_1(t)y_1'(t) + u_2(t)y_2'(t) + \dots + u_n(t)y_n'(t)$$
$$\vdots$$
$$y_p^{(n-1)}(t) = u_1(t)y_1^{(n-1)}(t) + u_2(t)y_2^{(n-1)}(t) + \dots + u_n(t)y_n^{(n-1)}(t)$$
$$y_p^{(n)}(t) = (u_1(t)y_1^{(n)}(t) + u_2(t)y_2^{(n)}(t) + \dots + u_n(t)y_n^{(n)}(t))$$
$$+ (u_1'(t)y_1^{(n-1)}(t) + u_2'(t)y_2^{(n-1)}(t) + \dots + u_n'(t)y_n^{(n-1)}(t))$$

Notice that we didn't apply our "condition" on the last expression. Our $n-1$ conditions are:

$$u_1'(t)y_1(t) + u_2'(t)y_2(t) + \dots + u_n'(t)y_n(t) = 0$$
$$u_1'(t)y_1'(t) + u_2'(t)y_2'(t) + \dots + u_n'(t)y_n'(t) = 0$$
$$\vdots$$
$$u_1'(t)y_1^{(n-2)}(t) + u_2'(t)y_2^{(n-2)}(t) + \dots + u_n'(t)y_n^{(n-2)}(t) = 0$$


Now let's plug these into our original equation:

$$y^{(n)}(t) + \dots + c_1y'(t) + c_0y(t) = g(t)$$

$$[(u_1(t)y_1^{(n)}(t) + u_2(t)y_2^{(n)}(t) + \dots + u_n(t)y_n^{(n)}(t))$$
$$+ (u_1'(t)y_1^{(n-1)}(t) + u_2'(t)y_2^{(n-1)}(t) + \dots + u_n'(t)y_n^{(n-1)}(t))]$$
$$\vdots$$
$$+ p_1(t)[u_1(t)y_1'(t) + u_2(t)y_2'(t) + \dots + u_n(t)y_n'(t)]$$
$$+ p_0(t)[u_1(t)y_1(t) + u_2(t)y_2(t) + \dots + u_n(t)y_n(t)]$$
$$=g(t)$$

We can rearrange the terms to get:

$$(u_1'(t)y_1^{(n-1)}(t) + u_2'(t)y_2^{(n-1)}(t) + \dots + u_n'(t)y_n^{(n-1)}(t))$$
$$u_1(t)[y_1^{(n)} + \dots + p_1(t)y_1'(t) + p_0(t)y_1(t)]$$
$$\vdots$$
$$u_n(t)[y_n^{(n)} + \dots + p_1(t)y_n'(t) + p_0(t)y_n(t)]$$
$$=g(t)$$

You might be able to notice something from the rearrangement of equations! We see a repetition of $y_i^{(n)} + \dots + p_1(t)y_i'(t) + p_0(t)y_i(t)$
multiple times! In fact, since $y_i$ for $i \in (1,n)$ are in fact our homogeneous solutions, they cancel out, simplifying our expression. thus we get:
$$u_1'(t)y_1^{(n-1)}(t) + u_2'(t)y_2^{(n-1)}(t) + \dots + u_n'(t)y_n^{(n-1)}(t)$$
$$=g(t)$$

Let's list out all of the conditions we have now:


$$u_1'(t)y_1(t) + u_2'(t)y_2(t) + \dots + u_n'(t)y_n(t) = 0$$
$$u_1'(t)y_1'(t) + u_2'(t)y_2'(t) + \dots + u_n'(t)y_n'(t) = 0$$
$$\vdots$$
$$u_1'(t)y_1^{(n-2)}(t) + u_2'(t)y_2^{(n-2)}(t) + \dots + u_n'(t)y_n^{(n-2)}(t) = 0$$
$$u_1'(t)y_1^{(n-1)}(t) + u_2'(t)y_2^{(n-1)}(t) + \dots + u_n'(t)y_n^{(n-1)}(t)$$
$$=g(t)$$

We have $n$ equations and $n$ unknowns for $u_i'(t)$. We can solve this! Let's put it in matrix form:

$$
\begin{bmatrix}
    y_1(t) & y_2(t) & \dots  & y_n(t) \\
    y_1'(t) & y_2'(t) & \dots  & y_n'(t) \\
    \vdots & \vdots & \ddots & \vdots \\
    y_1^{(n)}(t) & y_2^{(n)}(t) & \dots  & y_n^{(n)}(t) \\
    \end{bmatrix}
\begin{bmatrix}
    u_1'(t) \\
    u_2'(t) \\
    \vdots \\ 
    u_n'(t)
\end{bmatrix}
=
\begin{bmatrix}
    0 \\
    0 \\
    \vdots \\ 
    g(t)
\end{bmatrix}
$$


Notice our old friend, the fundamental matrix, again! We know it is in fact invertible since the \hyperref[th:EqLinInd]{Wronskian (the determinant) is non-zero since the homogeneous solutions we selected were linearly independent.}

Thus:

$$
\begin{bmatrix}
    u_1'(t) \\
    u_2'(t) \\
    \vdots \\ 
    u_n'(t)
\end{bmatrix}
=
\begin{bmatrix}
    y_1(t) & y_2(t) & \dots  & y_n(t) \\
    y_1'(t) & y_2'(t) & \dots  & y_n'(t) \\
    \vdots & \vdots & \ddots & \vdots \\
    y_1^{(n)}(t) & y_2^{(n)}(t) & \dots  & y_n^{(n)}(t) \\
\end{bmatrix}^{-1}
\begin{bmatrix}
    0 \\
    0 \\
    \vdots \\ 
    g(t)
\end{bmatrix}
$$

\section{\hyperref[th:wronAbelSys]{Proof of Abel's Theorem for systems}}
\label{sec:wronAbelSys}

Say we have the Wronskian of two solutions to a differential system:

$$
W(t) =
\text{det}\left(
\begin{bmatrix}
    x_1^{(1)} & x_1^{(2)} \\
    x_2^{(1)} & x_2^{(2)} \\
\end{bmatrix}\right)
=x_1^{(1)}x_2^{(2)}-x_1^{(2)}x_2^{(1)}
$$

Let's take the derivative of this and use the chain rule:

$$W'(t) = (x_1'^{(1)}x_2^{(2)}) + (x_1'^{(1)}x_2'^{(2)}) - (x_1'^{(2)}x_2^{(1)}) - (x_1^{(2)}x_2'^{(1)})$$

Rearranging terms:

$$W'(t) = [(x_1'^{(1)}x_2^{(2)}) 
- (x_1'^{(2)}x_2^{(1)})]
+ [(x_1'^{(1)}x_2'^{(2)})  - (x_1^{(2)}x_2'^{(1)})]$$

This is just (verify):

$$
W'(t) =
\text{det}\left(
\begin{bmatrix}
    x_1'^{(1)} & x_1'^{(2)} \\
    x_2^{(1)} & x_2^{(2)} \\
\end{bmatrix}\right)
+
\text{det}\left(
\begin{bmatrix}
    x_1^{(1)} & x_1^{(2)} \\
    x_2'^{(1)} & x_2'^{(2)} \\
\end{bmatrix}\right)
$$

Furthermore, from our initial system:

$$\mathbf{x'} = \mathbf{Px}$$
we see that:
$$x_1' = p_{11}x_1 + p_{12}x_2$$
$$x_2' = p_{21}x_1 + p_{22}x_2$$

Putting this into our initial expression for the derivative of the Wronskian:

$$
W'(t) =
\text{det}\left(
\begin{bmatrix}
    p_{11}x_1^{(1)} + p_{12}x_2^{(1)} & p_{11}x_1^{(2)} + p_{12}x_2^{(2)} \\
    x_2^{(1)} & x_2^{(2)} \\
\end{bmatrix}\right)
$$
$$
+
\text{det}\left(
\begin{bmatrix}
    x_1^{(1)} & x_1^{(2)} \\
    p_{21}x_1^{(1)} + p_{22}x_2^{(1)} & p_{21}x_1^{(2)} + p_{22}x_2^{(2)} \\
\end{bmatrix}\right)
$$

We can break this apart even more by using properties of determinants:

$$
W'(t) =
\text{det}\left(
\begin{bmatrix}
    p_{11}x_1^{(1)} & p_{11}x_1^{(2)} \\
    x_2^{(1)} & x_2^{(2)} \\
\end{bmatrix}\right)
+
\text{det}\left(
\begin{bmatrix}
     p_{12}x_2^{(1)} &  p_{12}x_2^{(2)} \\
    x_2^{(1)} & x_2^{(2)} \\
\end{bmatrix}\right)
$$
$$
+
\text{det}\left(
\begin{bmatrix}
    x_1^{(1)} & x_1^{(2)} \\
    p_{21}x_1^{(1)} & p_{21}x_1^{(2)} \\
\end{bmatrix}\right)
+
\text{det}\left(
\begin{bmatrix}
    x_1^{(1)} & x_1^{(2)} \\
    p_{22}x_2^{(1)} &  p_{22}x_2^{(2)} \\
\end{bmatrix}\right)
$$

The rows of some of the determinants in the expression above are just scalar multiples of other rows and therefore are 0 (because of linear dependence). Thus, the expression simplifies to:

$$
W'(t) =
\text{det}\left(
\begin{bmatrix}
    p_{11}x_1^{(1)} & p_{11}x_1^{(2)} \\
    x_2^{(1)} & x_2^{(2)} \\
\end{bmatrix}\right)
+
\text{det}\left(
\begin{bmatrix}
    x_1^{(1)} & x_1^{(2)} \\
    p_{22}x_2^{(1)} &  p_{22}x_2^{(2)} \\
\end{bmatrix}\right)
$$

We also know that we can take out a constant multiple of a row:
$$
W'(t) =
p_{11}
\text{det}\left(
\begin{bmatrix}
    x_1^{(1)} & x_1^{(2)} \\
    x_2^{(1)} & x_2^{(2)} \\
\end{bmatrix}\right)
+
p_{22}
\text{det}\left(
\begin{bmatrix}
    x_1^{(1)} & x_1^{(2)} \\
    x_2^{(1)} &  x_2^{(2)} \\
\end{bmatrix}\right)
$$

This is just:

$$W'(t) = (p_{11} + p_{22})W$$

When we integrate both sides, we get:

$$W = c\exp{\int{p_{11} + \dots + p_{nn})dt}}$$


\section{\hyperref[sec:center]{Proof that purely imaginary Eigenvalues form Ellipses}}
\label{sec:prCenter}

When we have a purely imaginary conjugate pair of eigenvalues, we know from our \hyperref[th:compEigenvals]{previous discussion} that our two real solutions are given by the real and imaginary parts of:

$$\mathbf{v_1}e^{\lambda t}$$

where $\mathbf{v_1}$ is an eigenvector.

Say that:

$$\lambda = i\mu$$
$$\mathbf{v_1}
= \begin{bmatrix}
    a + bi \\
    c + di \\
\end{bmatrix}$$
Then our complete solution is described by:

$$\mathbf{x(t)} = c_1\Re{\mathbf{v_1}e^{\lambda t}} + c_2\Im{\mathbf{v_1}e^{\lambda t}}$$

Simplifying:

$$\mathbf{x(t)} = c_1\Re{\begin{bmatrix}
    a + bi \\
    c + di \\
\end{bmatrix}
e^{i\mu t}} + c_2\Im{\begin{bmatrix}
    a + bi \\
    c + di \\
\end{bmatrix}
e^{i\mu t}}$$

Applying Euler's formula:

$$\mathbf{x(t)} = c_1\Re{\begin{bmatrix}
    a + bi \\
    c + di \\
\end{bmatrix}
(\cos(\mu t) + i\sin(\mu t))}
$$
$$+ c_2\Im{\begin{bmatrix}
    a + bi \\
    c + di \\
\end{bmatrix}
(\cos(\mu t) + i\sin(\mu t))}$$

which after expanding is just:

$$\mathbf{x(t)} = c_1\Re{
\begin{bmatrix}
    (a\cos(\mu t) - b\sin(\mu t)) + i(a\sin(\mu t) + b\cos(\mu t) \\
    (c\cos(\mu t) - d\sin(\mu t)) + i(c\sin(\mu t) + d\cos(\mu t) \\
\end{bmatrix}}
$$
$$+ c_2\Im{
\begin{bmatrix}
    (a\cos(\mu t) - b\sin(\mu t)) + i(a\sin(\mu t) + b\cos(\mu t) \\
    (c\cos(\mu t) - d\sin(\mu t)) + i(c\sin(\mu t) + d\cos(\mu t) \\
\end{bmatrix}}
$$

$$= c_1
\begin{bmatrix}
    a\cos(\mu t) - b\sin(\mu t) \\
    c\cos(\mu t) - d\sin(\mu t) \\
\end{bmatrix}
+ c_2
\begin{bmatrix}
    a\sin(\mu t) + b\cos(\mu t) \\
    c\sin(\mu t) + d\cos(\mu t) \\
\end{bmatrix}
$$

\hyperref[th:sinPlusCosin]{Since both components are just sums of cosines and sines of the same frequency}, we know that this evaluates to:

$$
\mathbf{x(t)} = 
\begin{bmatrix}
    A_1\cos(\mu t - \phi_1) \\ 
    A_2\sin(\mu t - \phi_2) \\
\end{bmatrix}
$$

(A sine function is just a shifted cosine ).

This is the end of our proof.
\end{document}